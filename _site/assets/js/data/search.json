[
  
  {
    "title": "Attack Simulation, Red Atomic Team",
    "url": "/posts/red-atomic-team-attack/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, windows, fleet server, elastic agent, endpoint",
    "date": "2025-05-29 12:33:00 +0700",
    





    
    "snippet": "UNDER CONSTRUCTION  UNDER CONSTRUCTION, PLEASE WAIT FOR THE UPDATE.",
    "content": "UNDER CONSTRUCTION  UNDER CONSTRUCTION, PLEASE WAIT FOR THE UPDATE."
  },
  
  {
    "title": "Integrating Fleet Server And Preparing Windows VM",
    "url": "/posts/integrating-fleet-server-and-preparing-windows-vm/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, windows, fleet server, elastic agent, endpoint",
    "date": "2025-05-20 15:15:00 +0700",
    





    
    "snippet": "A Little ReminderThis is part two, continuation from my post where we walk through installing ELK stack SIEM. On this part two, we will take a look at how to integrating our ELK stack with endpoint...",
    "content": "A Little ReminderThis is part two, continuation from my post where we walk through installing ELK stack SIEM. On this part two, we will take a look at how to integrating our ELK stack with endpoint, in this case is a Windows VM.We will learn also what is Filebeat, Fleet Server, Elastic Agent, Elastic Integration, and many more tools for getting our lab work as a complete eco system for simulating SOC monitoring, log capturing, threat hunting, and maybe many more in the future. Without further a do, let’s get into it.Creating Administrator UserFor this section we will create administrator user so that instead using superuser account all the time. The reason being is for best practice about privilege(cmiiw). Alright, let’s just login to Kibana first using our superuser like in the screenshot below.Login To Kibana Using SuperuserWith superuser account, we could login to Kibana and start to make new user. In the screenshot shown below we will be greeted with Home page menu, don’t worry if it’s just showing template menu, that is just how it is for fresh installment. Now click on the Hamburger Menu on the left side and there will be more sub-menu. We need to click on Stack Management menu.Heading To Stack Management MenuAfter that, we will be directed to the Stack Management page. Here, there’s so many options for you to set but for now let’s just click on the Users sub-menu on the Security category.Click on Users Sub-menuNext there will be Users being shown. In here, listed various users, each with it’s own roles, status, and etc. Want we want to do here is click on Create user button on the upper right corner.Click on Create userWith that, we will then begin to inputting necessary properties for this planned administrator user. There’s various values and properties that you could play around if you want to set something in the future but for now we just focused on these properties in the screenshot below.Setting Up New Administrator UserWhat basically say to Kibana is that we want this newly created user to have username kibana-lab, password that we set, and the privileges such as beats-admin, kibana_admin, and superuser. From the looks of it, the selected roles is somewhat self-explanatory but if you want to know more of it, here’s the full documentation of it. And with that we could login to the new created user.Creating And Integrating Fleet ServerNow that we have new administrator user, we then could login onto it and start integrating Fleet server to our Ubuntu Server machine. This is where our Elastic agents being managed, more about fleet server could be found here.To add our new Fleet server just head to Management sub-menu inside the hamburger menu and click Fleet just like screenshot below.Fleet Server PageFleet server is need some time to load but once its done, click on Add Fleet Server button, it will then navigate us to this page where we could download Fleet Server installer.Screenshot below shows us that we first need to download the file, for next is to set the ELK stack to Production mode because we have our own certificates that we generated on previous tutorial. And on the fourth steps, we specified the location of where we want to attached our Fleet server host. In here, I insert the address of our Ubuntu Server machine and put it in port 8220 and then clickAdding Fleet ServerNow we go back to our Ubuntu Server VM, head to cd /etc/fleet and insert this command to download the Fleet server:sudo wget https://artifacts.elastic.co/downloads/beats/elastic-agent/elastic-agent-7.17.6-linux-x86_64.tar.gzDownloading Fleet Server/Elastic Agent Installer FiLeAfter done downloading it, we need to extract it using tar command:sudo tar -xvf ./elastic-agent-7.17.6-linux-x86_64.tar.gzExtracting Fleet Server/Elastic Agent InstallerAnd this will make directory named elastic-agent-7.17.6-linux-x86_64, but if you like me and somehow the files is on the Desktop directory of your linux unlike nubb tutorial. Then follow the screenshot below.Moving Fleet Server/Elastic Agent Installer FolderHere’s the steps that you could follows based on that screenshot  ls, I first checking on the files or folders my current directory. With it I could see there’s only 2 inside, which is just tarball of the installer file and the extracted file.  sudo mv elastic-agent-7.17.6-linux-x86_64/ /etc/fleet, is to move the extracted folder to /etc/fleet. Previously failed attempt happen because I don’t include sudo to elevate our privilege.  cd /etc/fleet/elastic-agent-7.17.6-linux-x86_64/, to navigate inside it.After we successfully navigating inside the installer folder, we then could insert these commands shown below.Generating Fleet Server Installation Command And TokenOn the screenshot we could see the command being:sudo ./elastic-agent install --url=https://YOUR_OWN_IP:8220 \\  --fleet-server-es=https://YOUR_OWN_IP:9200 \\  --fleet-server-service-token=YOUR_SERVICE_TOKEN \\  --fleet-server-policy=499b5aa7-d214-5b5d-838b-3cd76469844e \\  --certificate-authorities=\"/etc/fleet/certs/ca/ca,crt\" \\  --fleet-server-es-ca=\"/etc/elasticsearch/certs/elasticsearch.crt\" \\  --fleet-server-cert=\"/etc/fleet/certs/fleet.crt\" \\  --fleet-server-cert-key=\"/etc/fleet/certs/fleet.key\"Keep in mind, dont FORGET change the all of the IP’s to YOUR OWN IP. If we don’t do that, the server won’t be able communicate with the ELK stack. Here’s what it would look like on the terminal.Fleet Server/Elastic Agent Installation ProcessLast steps is to make sure the newly made Fleet server is able to communicate with the rest of our ELK stack. With that being set, go to Fleet page once again and click on the Fleet Settings on the upper right corner of the page. We then being showed this window.Setting Fleet Server AddressMake sure your setting is matched with your own IP, otherwise, the Fleet server will not showing any captured logs like this screenshot below.False IP Address Causing No Logs AppearingAnd lets check on the Fleet server page, it should be succeeded by now.Fleet Server/Elastic Agent Installed SuccessfullyAnd we finally able integrating the Fleet server with our Ubuntu Server machine.Creating Agent Policies And Adding IntegrationNow that the medium to gather and managed the all of the logs has been created, we need to create utilization in order for us to able collecting the logs right? And that is what Agent Policies are, they are a set of input that has been defined in order collecting data according to their configuration. The agents also worked with Filebeat to get specified logs and could be set working in tandem for log ingestion.Before we continue further let’s check on our Fleet server status and see what it’s look like.Fleet Server StatusThe status of our Fleet server is healthy, that itself is self explanatory but there’s also several status such as Unhealthy, Offline. Next we will be adding new policy.Now for us to be able making this policies we need to navigate by clicking Create Agent Policy tab on the Fleet page and then we will be shown this page.Fleet Agent Policies TabOn the image, we also seeing the default policies but what we need is our own custom policy and for that just click on the Create agent policy button for then will be showed Create agent policy window.Creating Agent PolicyI will be naming this policy Windows-Endpoint because we will be putting it on windows endpoint machine for collecting logs. And for the description, fills whatever suits your need. After all of that done, just click Create agent policy. After that the policy will be show up on the page now.And after the policy is available, click on it to configure of basically what integrations we will be putting inside of that policy.Selecting Newly Created PolicyNow we’re in the list page of all the integrations that is inside the Windows-Endpoint. In here we can see various integrations that available, right now only System integration that is available but we will add another integration. Just click on Add integration.Add IntegrationWhen we clicked on it, there will be Browse Integrations page and in here we can search also choose what integration that we would like to add. For us now, we need to search Endpoint Security for us to be able securing windows endpoint, so use the search button to search for it. Once we got the result, just click on it to set it up.Searching Endpoint Security EndpointNow we’re able to configure it to our need, I’ll name it windows-edr-integration, fill the description and lastly choosing previously made policy Windows-Endpoint.Endpoint Security IntegrationAfter all that done, just wait a moment and there will be success notification popped up.Successfully Adding Endpoint Security IntegrationThe window shows up, we will then click on the Add Elastic Agent later button and continue to adding also configuring another integration. The integration this time is Windows integration, that way it will help us collecting logs from our Windows machine. Just repeat the previous steps for adding it and then we will arrived in the setting window just like before.Configuring Windows IntegrationI just named it windows-log-collect for the sake of convenient and then choosing Windows-Endpoint policy. After that, continue to scroll down until you see Show advanced setting and click on it. We will then presented with various input column, we want to search for windows.advanced.elasticsearch.tls.ca_crt properties and in it we will paste C:\\Windows\\System32\\ca.crt path. This path will later be used when we copying the CA certificate from our linux VM to the Windows, but for now just set it. And we done for the Windows integration, just click Save and continue. We done with the needed integration, the integration should be ready to integrated to the target machine but because we don’t install any Windows VM yet, the next step is to install Windows VM.Preparing Windows VMHere comes another VM installation part, so let get this over quickly because it’s basically just the same stuff with the Linux installation.  First we want to download the ISO for Win10, to do that just go to multi-edition download site. Just follow along the instruction on the website.  Now that we downloaded media installation tool, run it and we will be shown window for us choosing what kind of action we want for the media creation tool. We want it to make ISO image for the Windows 10.Choosing Windows 10 Image Format  After the format has been chosen, we then select ISO file to be the media of our image. If you want it, you could also choose USB drive but for now we want it to be a file for our VirtualBox.Creating ISO Image  The EULA pop up, just scroll it until done and click Accept.Accepting EULA  Now we need to specify where we need to put the ISO file on our computer and don’t forget to naming it. With that done, the download then begin.Specifying File Location  When the download is done, head to VirtualBox and then click on Machine &gt; New and then select the newly downloaded Windows ISO image.  Don’t forget to uncheck the Unattended install and begin to set your system requirement, I’ll go with RAM: 4GB, CPUs: 2, Storage: 35 GB.  Now just following along the installation instruction but when you arrived at Which type of installation do you want? part, just choose the Custom: Install Windows only (advanced).  For the allocation of the partition, just use all of it but again, just go with whatever suit you if you want to play around with it.  And just wait for the installation to complete. After it finished, I suggest you disabled your internet momentarily. Why? Because the Windows will bothering you with all of its bullshit setup like Cortana(fucking hate this, it just bloat the Windows), especially they forcing you to make Windows account and we don’t want that. Disabling the internet just skipping all of that hassle. There’s other method tho like inputting Shift + F10 and oobe/bypassnro in the CMD. But for me it’s not working, so I just go with simplest solution.  And now, we done installing Windows VM, here’s what it look like.Windows VM  For me, we not done yet. There’s still one more thing to configure. And that is adding Guest Addition CD image. This adds makes us enable to fixing screen display(I don’t like the display), making us seamlessly changing screen(ALT+TAB), and many more. Just click Devices &gt; Insert Guest Additions CD image.Windows VM  After we done clicking that, there will be ISO image mounted on the Windows machine, just double click on it.Insert Addition Image  Inside the image, we will presented with various installer. Just double click on VBoxWindowsAddition.exe to begin the installation. Just for the installation to complete and after it complete, you will be asked to reboot the Windows VM. Just follow along.Mounting Addition Image  We then could check whether the addition image is installed or not by navigate to View and see if the Virtual Screen 1 has options for various screen resolution. And we done for the VM preparation.Windows Endpoint Preparation For Elastic AgentNow we can start by testing the connection of the Windows VM to the Ubuntu Server VM, where our ELK stack installed.We start by testing connection alongside the port whether it is listening or not. Just insert command Test-NetConnection -port INSERT_RESPECTIVE_PORTS YOUR_OWN_IP(remember to change it to your own IP and port that you want to test).Checking Connection To ELK StackNow we know the connection is succeeded, we then could proceed to installing Sysmon, basically a service for monitoring logs activity of our windows machine. We need to download it through PowerShell as Administrator and then insert these command:  Invoke-WebRequest -URI https://download.sysinternals.com/files/Sysmon.zip -OutFile \"C:\\Program Files\\Sysmon.zip\"  Expand-Archive \"C:\\Program Files\\Sysmon.zip\" -DestinationPath \"C:\\Program Files\\Sysmon\"Downloading SysmonBasically what all of those mean is us to download Sysmon, put it on C:\\Program Files, and extracting it. Now that we got the file, we could then cd \"C:\\Program Files\\Sysmon\" and insert the command:  Invoke-WebRequest -URI https://raw.githubusercontent.com/SwiftOnSecurity/sysmon-config/master/sysmonconfig-export.xml -OutFile \"C:\\Program Files\\Sysmon\\sysmonconfig-export.xml\"  ./sysmon.exe -accepteula -i sysmonconfig-export.xmlExecuting The Template ConfigurationBasically the commands says that we downloading template of Sysmon configuration by github user SwiftOnSecurity and here’s the link of the template.Now we have the template, we need a script to adjusting it, so just enter this Shell command:function Enable-PSScriptBlockLogging{$basePath = 'HKLM:\\Software\\Policies\\Microsoft\\Windows' +'\\PowerShell\\ScriptBlockLogging'if(-not (Test-Path $basePath)){$null = New-Item $basePath -Force}Set-ItemProperty $basePath -Name EnableScriptBlockLogging -Value \"1\"}The script basically checking whether if there’s registry key exist or not and if it’s not it will set the boolean condition to True with value 1.Enrolling Elastic AgentThis part where we start by transferring our CA certificate from Ubuntu Server to our Windows but first for extra measure, head to cd /usr/share/elasticsearch and copy ca.crt from there to home directory using sudo cp ca/ca.crt /. If the file is already there then it will showing notification that says there’s already same file but if it’s not then it will succeeded. Next thing to do, we just need enter this command inside the PowerShell as Administrator.scp YOUR_UBUNTU_USERNAME@YOUR_OWN_IP:/ca.crt ./ Copy CA Certificate From Ubuntu ServerThat command telling PowerShell to use secure file transfer protocol to copy a file from the Ubuntu Server VM. And now we will do the same with the within the Ubuntu Server that is to insert this command,  scp YOUR_UBUNTU_USERNAME@YOUR_OWN_IP:/ca.crt /usr/local/share/ca-certificates/  sudo update-ca-certificatesIt’s just copying file but this time we want to update our certificate in case it still uses old certificate.Copying CA Certificate For Linux Ubuntu ServerDon’t mind the failed first try command, that is happen because I was careless and not include sudo on my command.All that left is to download Elastic agent on our Windows VM:  Start-BitsTransfer -Source https://artifacts.elastic.co/downloads/beats/elastic-agent/elastic-agent-7.17.6-windows-x86_64.zip -Destination \"C:\\Program Files\\elastic-agent.zip\"  Expand-Archive \"C:\\Program Files\\elastic-agent.zip\" -DestinationPath \"C:\\Program Files\\elastic-agent\"Download Elastic Agent For WindowsThose two commands is just us telling Windows to download it and extracting Elastic agent file on the given directory. With that, Elastic agent installer is on our machine, then we would like to return to Fleet page in the Kibana, this time we will adding agent. The page would look like this.Adding Agent To WindowsThis page is inside Fleet &gt; Agent Policies Tab &gt; Windows-Endpoint(or your own named Windows endpoint policy). In here we click on Add agent on upper menu. After we click on it, there will be window shows up on our right screen that will instruct us on how to adding agent to our endpoint.Add Agent InstructionWe simply just generate our own enrollment token, also because we already downloading agent installer we just skip to next step that is installation command.Enrollment Installation ComamandWe inserting this command on our PowerShell, basically this command instructing the installer to installing Elastic agent with our address along with attached port, and it’s own enrollment token. And don’t forget to change to it’s directory!.  If your PowerShell location outside on any other directory, insert this command cd C:\\\"Program Files\"\\elastic-agent\\elastic-agent-7.17.6-windows-x86_64. You could replace the 7.17.6 with any other version that you installed.  .\\elastic-agent.exe install --url=https://YOUR_OWN_IP:8220 --enrollment-token=YOUR_OWN_TOKEN. You could also add another flag on that installation command --certificate-authorities=\"C:\\Windows\\System32\\ca.crt\", but for me I just go along with the command given by the Elastic.Enrollment Installation CommandAnd that’s it! Just wait a bit for the installation to finish itself and you got your endpoint integrated with Elastic agent policy. Let’s check on it, see if it’s really installed.Agent Installed SuccessfullyWe finally Done! What a long way huh, you could use this step to add another agent on different endpoint, adding more integration, making another policy and just explore on your own. I really hope this tutorial make you learn something. Before I finish this tutorial, let’s see what would our Kibana catching logs look like.Logs Appearing On KibanaAnother moment of truth, let’s what our progress done for us now. Click on the hamburger menu button, under Kibana menu section click Discover and now you will be shown dashboard.Captured LogsFinal WordsAnd we officially done with the installation series. From here on, you could start your own exploration, see what kind of scenario you want to set up or you could follow part 4 of LevelEffect by William Nissler. This guide is meant to teaching you guys how to catch the fish from my POV. You could also add your own twist but for me the basic stays the same.So again, thank you so much for reading and following this guide and hope you learning something."
  },
  
  {
    "title": "ELK Installation With Two VMs For Home Lab",
    "url": "/posts/elk-installation-with-two-vm/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, windows, linux, virtualbox",
    "date": "2025-05-20 13:21:00 +0700",
    





    
    "snippet": "Before We StartThis section is a explaination for what to expect in this blog and also serve as a shoutouts to all of the peoples that making knowledge in form of tutorials. With that being said, t...",
    "content": "Before We StartThis section is a explaination for what to expect in this blog and also serve as a shoutouts to all of the peoples that making knowledge in form of tutorials. With that being said, this is some of the people that helps me on this installation:  crin, thank you for making discord server where newbie like me and others could gather and share knowledge.  nubbieeee, thank you so much for reaching out to me on discord. Really appreciate your guide, with it  I’m able to present my progress.  Will Nissler from LevelEffect, thank you for making the tutorial that I could use as a main guideline alongside with nubb tutorial. Your tutorial really have a lot of information.  And other tutorial that I will list on the reference that helps me fix certain error and circumstances that I experience along my installation and setup progress.What To Expect From This GuideThis guide is served as a supplementary and a note for present and future me or maybe others that will or want to start installing Elastic for home lab where it could be used as tools of experimenting and hands on learning of cyber security. By no means this guide is perfect and if you happen to encounter any error, use the material that I listed on the refence section or like the old saying Just google it. Because at the end of the day, cyber security is heavily relied on the so called professional googling. And don’t forget to ask ChatGPT, it really helps to direct you to more accurate answer.I’m planning to make this guide into two parts or maybe more depending on my current assignment given to me. But for the time being it will be just two part. For this part one, I will focusing on installation and setting up the ELK stack and all of the VMs.Some Stuff That You Will LearnThere are some stuff here that I sure you will learn one or two when following this guide, some of it are:  Setting up and configuring VM with VirtualBox.  A bit of networking stuff such as port forwarding, networking.  ELK stack installation and understand how they work.And maybe many more that I couldn’t mention above. Hope this guide become useful for me in the future or for you guys too.Objective of This GuideOkay this will be the last I’ll yap other than the actual guide. So here’s some of the objective for part one of this guide series:  Setting up all of the VMs (i.e. Linux and Windows) to suit the requirement.  Setting up VMs connection configuration.  Installing and configure Elasticsearch on Ubuntu server VM.  Installing and configure Kibana on Ubuntu server VM.  Installing and configure Filebeat on Windows VM.  Successfully login to ELK stack after the installation.Let’s BeginNow in this section, we will begin the installation progress by first understanding what is ELK stack and then understand what this lab looks like on a diagram to further enhance our understanding of this lab. First thing first, what exactly is ELK stack? I’ll add my 2 cents here too since the official documentation already done such great detail in explaining it. Basically ELK is an acronym, Elasticsearch Logstash Kibana and each of that technology is serve their on purposes.Elasticsearch is to search, analyze, and process all of the data that being collected. You could call it the engine of ELK stack and Logstash is the instrument that ingest all of that data from multiple sources to Elasticsearch and when the data arrived in Elasticsearch, it is then can be visualize by Kibana for making user more easy to understand all of those data in form of charts and graphs.DiagramAnd with that explanation in mind, we then could utilize ELK stack as a SIEM to monitor our system. For the system of this lab, I will explain it with the diagram below to better understand the context of what it looks like:Here’s a simple diagram to illustrate our labI admit, it’s not fancy but for me it could convey what our lab will look like and this is as far as my understanding of the lab. But if you want a more detailed overview of the diagram, head to William Nissler guide he got to more detailed about it. So from that diagram, we want to install ELK stack on the ubuntu server with the listed ports and that port will be attached to the DHCP IP.And the ELK stack will receiving log mainly from the Windows VM through sysmon, filebeat, windows event log, and powershell script. And also the Windows VM having the IP received from the DHCP process that can be used to make connection between two VMs. Lastly, the Windows analysis machine is our own PC that act as a monitoring device that can access the ELK stack with the localhost ip. But why localhost you say? To put it simply, we can’t just access the VMs directly because the nature of VM itself.VM is run on their own separate kernel despite it’s still in our machine and that also mean they having their own network too, that is why in the diagram I put router to illustrate this process as router is the device that handling layer 3 i.e. network(IP, etc). And to access that network we have to set the NAT and some ports too if we have to use some protocol. We will go through this more in depth in the next section. But for now, just know that we can’t just access the VMs directly.Linux Ubuntu Server VM InstallationIn this section we will begin first with installing Ubuntu Server for our ELK stack. The Ubuntu Server version that I and some of the guides that I used is 18.04 LTS, the LevelEffect guide didn’t mention any reason as to why choosing this version but seeing the time of the blog being published was in 2022, my guess is version 18.04 is more stable than any version during that time.And if we done choosing the version, here’s some requirement for the Ubuntu VM:            Hardware Requirement                         OS      Ubuntu Server 18.04              RAM      4 GB              CPU      2 Cores              Storage      30 GB      I go with the requirement above because it suits my need and my hardware capability but if guys want to tweak some of the requirement then go ahead. With that being said, the steps for installing Ubuntu server is as follows:  Download it from the official Ubuntu server website.  After done downloading it, we then open VirtualBox and click Machine menu and select New….Selecting Machine To Create New VM  It will then having Create Virtual Machine pop up, with it we could configure and set the properties of the VM. Just put the machine hardware just like our hardware requirement previously. One thing to note here is that, I checked the Skip Unattended Installation because if we want to use unattended installation that mean VirtualBox will handles the installation based on the provided setting and in this case we want to configure it manually.Configuring VM Hardware  Next, the VM will appear on the VirtualBox menu manager, select that and click on Start icon to start the VM installation.  The VM starting and next thing is to select language, select whatever language that suits you but in this case I choose english for obvious reason.  The language set is done, now we will greeted by Ubuntu Server version notification. For our case, just select Continue without updatingSkipping Version Update Selection  Here, there will be IP that we got from DHCP, note it because this is important for basically every next configuration.Receiving IP DHCP  If being asked to choose proxy on the next step, just skip it because we don’t need it.  Also just go with the default mirror link. Basically this is just Ubuntu asking for sources to download its dependencies and packages.  The next menu is storage partition. For me personally I’ll just go with the default but feel free if you guys want to get creative with it.  Okay, next is configuring username and password for the Ubuntu. Remember also to make password that easy to remember but also not too easy to guess(it will be used a lot in the next steps). For this section, I’ll configure it like shown below but then again, feel free just go with something you guys want.Creating Account For Ubuntu  The account has made, we then continue to enabling SSH for this Ubuntu.Selecting Enable SSH  All of that has been done then we wait for the installation to finish. It will somewhat look like this or similar.Ubuntu Server Successfully InstalledNow with the installation complete I’ll add one more important thing to do before we proceed further. I want you guys make snapshot for each section of this guide. Yep, this will serve as a failsafe incase something went wrong, making it “point of return” for us(yes pun intended). Here’s how to do it:  Click on Machine menu again and select Take Snapshot…Selecting Take Snapshot  After that there will be menu pop that want us to name the snapshot. The name will function as a description for what exactly is this snapshot that we make. So make sure the name and description is informative about what machine situation when the snapshot being made. Making Snapshot Name and DescriptionAnd with that our Ubuntu Server is done we could continue to next step, that is updating our Ubuntu with this commands. But we need to login to our Ubuntu. After done login, execute these steps:Disclaimer for sudo CommandFrom here on we will using A LOT really a lot sudo and it will often asking to enter our root password(same with your user password) that is why I said to use easy to remember password previously. Basically we want to minimize access to our root as little as possible because security concern(shoutout to nubb, thanks for the insight). Okay, on with the guide.  Enter command sudo apt update &amp;&amp; sudo apt upgrade -y. These commands will update and upgrading our existing packages.  Next command sudo apt dist-upgrade -y. Same with previous but the difference is, this command will handle changes of the packages if necessary.  sudo apt install zip unzip -y is the next command, this command installing zip and unzip application for us to be able extracting zip format file and compressing file into it.  Lastly there’s sudo apt install jq -y that used for JSON parsing and transformation. It’s just a fancy words that means our Ubuntu able to interact with JSON format file.With all of that being done, our Ubuntu is ready. Oh, and don’t forget to make snapshot here too just to be extra careful if you guys want.Accessing Machine Through SSHChecking Ubuntu SSHThis section will focusing on SSH thingies, starting from checking it’s status, setting up NAT rules, and many more. I recommend using SSH to simulate remote connection within work environment and to avoid hassle of interacting directly with the VirtualBox(yes, it’s such a pain in the ass). But first thing first, login to the Ubuntu VM and execute this steps:  First, we have to check our Ubuntu SSH status. There’s some possibilities here, either our SSH is working or not. Let’s make sure of it with this command, sudo systemctl status ssh. On my case, the SSH is Active and running like shown below.Making Snapshot Name and DescriptionThis is because we enabling SSH previously during our installation but in case if some of you guys seeing failed or dead status proceed to next steps.  Execute this command sudo apt-get remove --purge openssh-server &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install openssh-server. That commands(I use plural here because there’s two command that being fused together with &amp;&amp; operator) basically the commands will first remove and deleting all of the SSH files, service, etc and then installing it again.  When it is done, you guys could run sudo systemctl status ssh and you should see a dead status but don’t worry, execute sudo systemctl enable ssh to enable the service and then systemctl start ssh to start it again. And now next check it’s status again, it should be running now. Don’t forget to make snapshot 📷 .Network And SSH ConfigurationEven with all of that progress previously, we still couldn’t even access the VM directly by their IP, nuh uh that would be too convenient. Welcome to IT world, where if it’s not pain in the ass you should suspect somethings wrong. So how do we SSH the VM then? well let’s get into it:  First head to our VirtualBox Manager menu and select the network option shown below.Selecting Network Menu      After that there will be network menu, in it select NAT Network tab and click on Create icon to create new NAT Networks option.Configuring NAT NetworkFrom the image above, we basically configuring our NAT Network to named “ElasticNetwork” to the network IP within prefix of 192.168.68.0/24 with DHCP server enabled. Below it, we could see there’s Port Forwaring tab where you could add by clicking on the green plus icon and set it’s name, the protocol which is TCP for SSH, host IP being left blank because we want it to be able receiving any IP other from the prefix range, the host port, guest IP with our own Ubuntu IP previously, and lastly the guest port being 22.    Next is to set our network mode for our Ubuntu VM, again we head to Machine &gt; Settings on our VM windows and after that select Network like shown below.Setting Ubuntu Network ConfigurationChange the Attached to with NAT Network, name change it with the network that we make, and lastly I set the Promiscous Mode to Allow All. That make sure it could capture all traffic because well, this is a SIEM lab.Successfully SSH To Ubuntu VMNow all of that done, finally we are able to SSH to the VM and maybe some of you wondering why the IP is 127.0.0.1 and not 192.168.68.5? Well according to the guides that I read, this is possible because the port forwarding rules that we set before that allowing all IP to access SSH to Ubuntu VM through port 2222 which then being redirected to port 22. And 127.0.0.1 itself is another way of saying localhost. I’ve tried to access it with IP 192.168.68.5 but there’s no connection, I still dunno why is that happen even though I’ve tried to add rules for it¯\\_(ツ)_/¯. But oh well, maybe I’m just that noob. Oh and don’t forget to make snapshot 📷.Setting Up ElasticsearchOkay, now the nitty gritty part of it, buckle up and prepare to get your hands dirty because there will be a lot to configure. Oh and we will continue this part on SSH connection.Downloading and Installing ElasticsearchBut before we start downloading Elasticsearch, execute this command sudo apt install apt-transport-https -y to enabling us using APT transport for the HTTPS protocol which is a more secure version of HTTP and for the GPG key of the Elasticsearch. Again, some security stuffs and best practice.Okay, below is screenshot and steps for Elasticsearch installation:Successfully Downloading Elasticsearch  First we download GPG key for the Elasticsearch with command wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -  Next, to see if the repo is exist for our Ubuntu echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list.  Now, with all of that done we could update and install Elasticsearch sudo apt update -y &amp;&amp; sudo apt install elasticsearch -yDone and with that we could see the status of Elasticsearch service just like SSH previously with command sudo systemctl status elasticsearch.service.Checking Elasticsearch Service StatusAs you can see, the status is dead and that is because still haven’t enabling the service yet, that will be done on the next section.Configuring Elasticsearch And Checking It’s ConnectionBefore we enabling and running the service, let’s configure Elasticsearch first to suit our need like changing it’s address, protocols, ports, etc.We could do that by entering command sudo nano /etc/elasticsearch/elasticsearch.yml. Let me explain a bit that command, basically we accessing elasticsearch.yml with super user privilege, then choosing our text editor for mine it’s nano but you could choose other text editor, and lastly we specifying the filepath of elasticsearch.yml to be able to open it.After that done, you should see the text editor open up and you could start changing the properties such as shown below:Changing Elasticsearch PropertiesWhat we’re changing here is:  cluster.name to our own wanted name.  network.host to our own IP.  http.port is just commented out because this will be default value of 9200.  discovery.type is the last. This is for Elasticsearch to know whether we run a single node or multiple. More of it can be read hereWith this we then could do these following steps:Enabling Elasticsearch Service And Testing It’s Connection  We first enable the service with sudo systemctl enable elasticsearch.  Then we start it sudo systemctl start elasticsearch.  After that, checking its status sudo systemctl status elasticsearch.  Finally we check the connection with curl --get http://YOUR_OWN_IP:9200. Change YOUR_OWN_IP with the IP you got from DHCP previously.Setting Up KibanaNow, we do the same with Kibana, take a look this screenshot below.Successfully Installing KibanaEnter command sudo apt install kibana -y and wait for it to finish the process. After that we edit the Kibana YML configuration, enter command sudo nano /etc/kibana/kibana.yml disclaimer again, nano is my text editor choice.Changing Kibana YML PropertiesIn here, we change:  server.port, we just commented it out because we will be using it’s default properties.  server.host, change it again to your own IP.  server.name, change it to your desire.Kibana CredentialsAs for Kibana credentials, we just leave it for now. We will back to it in the later steps. And with that, let’s continue next step that is enabling Kibana service.Enabling Kibana Service  We always start with enabling service first, sudo systemctl enable kibana.  Then start it’s service sudo systemctl start kibana.  And finally checking it’s status with sudo systemctl status kibana.And that’s it, we finally installed Elasticsearch and Kibana. You could also checking Elasticsearch and Kibana status both at the same time with sudo systemctl status elasticsearch &amp;&amp; sudo systemctl status kibana. And as always don’t forget to take snapshot 📷.Important NoteThis section is for those who are done for the day of configuring this lab and want to take a rest or any other task. If you want to stop, you have to shutdown all of the component properly, here’s how you do it.Shutting Down System ProperlyThe command used is in order, so first it’s sudo systemctl stop kibana and the second is sudo systemctl stop elasticsearch. This way we could safely shut down our system without worrying the risk to break anything. And another note, kibana.service == kibana &amp;&amp; elasticsearch.service == elastic.And if you guys want to continue again:  sudo systemctl start elasticsearch  sudo systemctl start kibanaSetting Up FilebeatSame thing with previous installation steps, enter sudo apt install filebeat -y, wait for the process to be done and edit the YML file sudo nano /etc/filebeat/filebeat.yml.Inside filebeat.yml file we could adjust and change variety of input received by Elasticsearch from journald, winlog,  Windows event logs, and many more. Winlog helps us captures more log data beyond just logged events. The guides that I follow mostly using Filebeat to collect it’s log, so let’s just focus on that too.What you have to edit in the filebeat.yml is as followsChanging Filebeat HostsWe change that to our own IP that attached to Elasticsearch port 9200. And the next thing to is shown in screenshot below.Disabling Logstash And Checking The Connection To ElasticAccording to the guides that I follows, this is to disable Logstash and to ensure Filebeat correctly connected to Elasticsearch. We use sudo filebeat setup --index-management -E output.logstash.enabled=false 'output.elasticsearch.hosts=[\"YOUR_OWN_IP:9200\"]'.Next we enable the service sudo systemctl enable filebeat.service and we start it sudo systemctl start filebeat.service. Lastly we use curl --get http://YOUR_OWN_IP:9200/_cat/indices?v, that command simply making an API call of the URL and printing it out in verbose but also in column format. Making it more readable. The health here is just indicator of shards whether it’s condition is healthy, caution or danger but as far as I know, this not really mean much. So don’t mind it. Oh and don’t forget to take snapshot 📷.CA Certificate Authorities Configuration and Elastic IPBut why making CA certificate even though Elastic come with it’s own default certificate? To my knowledge the default protocol when access Elastic is HTTP but we want it to be HTTPS, so this is where CA certificate comes in handy. With it, we could enabling HTTPS connection.Now, without further a do, use cd /usr/share/elasticsearch to navigate to elasticsearch directory and enter sudo touch instances.yml and then open the text editor sudo nano instances.yml. After opening the file, insert this propertiesinstances:        - name: \"elasticsearch\"          ip:                  - \"YOUR_OWN_IP\"        - name: \"kibana\"          ip:                  - \"YOUR_OWN_IP\"        - name: \"fleet\"          ip:                  - \"YOUR_OWN_IP\"The IP inserted there is for fleet service able to reach out to it and make CA certificate according to the given IP there. Before exiting, save that file. And yeah, if you want, don’t forget to take snapshot 📷.Time to Create CA Certificate and Other CertificatesSo next is to generate and create other certificates as well, head to cd /usr/share/elasticsearch and we enter command sudo /usr/share/elasticsearch/bin/elasticsearch-certutil ca --pem. That basically mean we using executable file that generate the certs itself, and the encryption format of are X.509. Which is a public key certificates used in TLS/SSL, and HTTPS as well to secure the web, more about that can be found here. As for the --pem itself, it’s a standard file format of storing and sending encrypted key and certificate. And with that, we will get output as follows:CA Certificates OutputOne more thing to know based on that output, we using the instances.yml that made previously to help us generate these certificates. And we compressing it in zip file format, as for the file name I choose to name it elastic-stack-cat.zip but you can choose whatever name convenient to you. With it, we enter sudo unzip elastic-stack-ca.zip to unzipping the file.Nicely done, next steps is to put use of these certificates by using it to generate private key and another certificates. Use this command sudo /usr/share/elasticsearch/bin/elasticsearch-certutil cert --ca-cert ca/ca.crt --ca-key ca/ca.key --pem --in instances.yml --out cert.zip. Follow the screenshot below.Generating Private KeysI’ll go with cert.zip as the name but and I we also need to unzipping this file too. And after that we need to use command sudo unzip cert.zip to unzipping it.Unzipping Cert And Making Directory CertNotice that Linux making it’s own directory after unzipping it, because of that the next thing is to move all of those certificates to our newly created directory. We make directory to store all those certificates with sudo mkdir certs, after the directory created use sudo mv /usr/share/elasticsearch/elasticsearch/* certs/ first, then sudo mv /usr/share/elasticsearch/kibana/* certs/, lastly sudo mv /usr/share/elasticsearch/fleet/* certs/. After the copying steps done, we then use sudo mkdir -p /etc/elasticsearch/certs/ca &amp;&amp; sudo mkdir -p /etc/kibana/certs/ca &amp;&amp; sudo mkdir -p /etc/fleet/certs/ca to make CA own directory just like screenshot below where we could store our CA certificates.The CA directory has been made, now we copy CA certs and other certificates to their respective directory with these command.  sudo cp ca/ca.* /etc/elasticsearch/certs/ca &amp;&amp; sudo cp ca/ca.* /etc/kibana/certs/ca &amp;&amp; sudo cp ca/ca.* /etc/fleet/certs/ca  sudo cp certs/elasticsearch.* /etc/elasticsearch/certs/ &amp;&amp; sudo cp certs/kibana.* /etc/kibana/certs/ &amp;&amp; sudo cp certs/fleet.* /etc/fleet/certs/I used list command to check the directory and noticing that there’s still certs directories left around. So lastly just like mom used to say, clean up after playing with your toy and like a good kid we are we clean up our mess with sudo rm -r elasticsearch/ kibana/ fleet/Moving The Certificates FilesAlso don’t forget to take snapshot 📷 hehehehe.Best Practice For SIEM DeploymentThis section is for us to implement best practice when we deploying SIEM is to have least privilege within our systems and according nubb, the PrintNightmare threat is still lurking rampant out there so gotta be careful. More about that you could read here.And this also correlate with our usage of sudo command where we want the least privilege. With that in mind, head to cd /usr/share and execute commands:  sudo chown -R elasticsearch:elasticsearch elasticsearch/,  which will recursively set the ownership of the elasticsearch.  sudo chown -R elasticsearch:elasticsearch /etc/elasticsearch/certs/caThe first command let us set the elasticsearch directory to Elasticsearch and also change its group to elasticsearch that helps us limit permission to again, Elasticsearch. Second command is doing the same thing but for ca directory.Best Practice SIEM DeploymentBased on that screenshot, the command being used is sudo openssl x509 -in /etc/elasticsearch/certs/elasticsearch.crt -text -noout. Where there’s couple of command flag that I want to explain to the best of my understanding. -in flag used to specifying the certificate to check out, -text to make the output in readable format and lastly -noout is basically minimizing the gibberish encoded message to help further making it more readable.Setting Up HTTPS Connection With the CertificatesFirst head to the kibana.yml file with sudo nano /etc/kibana/kibana.yml and we copy this YML script below:server.ssl.enabled: trueserver.ssl.certificate: \"/etc/kibana/certs/kibana.crt\"server.ssl.key: \"/etc/kibana/certs/kibana.key\"elasticsearch.hosts: [\"https://YOUR_OWN_IP:9200\"]elasticsearch.ssl.certificateAuthorities: [\"/etc/kibana/certs/ca/ca.crt\"]elasticsearch.ssl.certificate: \"/etc/kibana/certs/kibana.crt\"elasticsearch.ssl.key: \"/etc/kibana/certs/kibana.key\"server.publicBaseUrl: \"https://YOUR_OWN_IP:5601\"xpack.security.enabled: truexpack.security.session.idleTimeout: \"30m\"xpack.encryptedSavedObjects.encryptionKey: \"min-32-byte-long-strong-encryption-key\"Remember to change YOUR_OWN_IP to your own IP and also some of you might notice that I input 10.0.2.15 as the IP right? Well that just me being dumbo and you will see what happen when you don’t correctly set the IP in the next section. And all of These properties is all within the kibana.yml but it will be pain in the ass to search each of it and editing them so yeah, just copy that. And it will look like this.Configuring Kibana SSLThe properties here somewhat self explanatory, the public base URL is our Kibana address, and others is for our encryption thingies.  But albeit, I agree with nubb here, I was also confused when first time handling the certificates and encryption stuff of ELK stack. But basically it’s just how Kibana telling Elastic that the communication between them is secured with various encrypted certificates and that Kibana make sure, it is trusted Kibana that talking with Elasticsearch.Or, cmiiw that is just how I understands these whole shenanigan. Proceed to next step that is basically the same but for elasticsearch.yml. sudo nano /etc/elasticsearch/elasticsearch.yml and start add the properties.xpack.security.enabled: truexpack.security.authc.api_key.enabled: truexpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificatexpack.security.transport.ssl.key: /etc/elasticsearch/certs/elasticsearch.keyxpack.security.transport.ssl.certificate: /etc/elasticsearch/certs/elasticsearch.crtxpack.security.transport.ssl.certificate_authorities: [\"/etc/elasticsearch/certs/ca/ca.crt\"]xpack.security.http.ssl.enabled: truexpack.security.http.ssl.verification_mode: certificatexpack.security.http.ssl.key: /etc/elasticsearch/certs/elasticsearch.keyxpack.security.http.ssl.certificate: /etc/elasticsearch/certs/elasticsearch.crtxpack.security.http.ssl.certificate_authorities: [\"/etc/elasticsearch/certs/ca/ca.crt\"]Configuring Elasticsearch SSLI’ll leave the official documentation for you to read more information and nubb explanation about this SSL things. And I agree again with nubb here, are the Elastic devs high on something when making the script to not use quotation for file path? Only God and them knows why.With that said and done, we have to sudo systemctl restart elasticsearch and next sudo systemctl restart kibana on respective order for enabling us applying the changes. With that don’t forget to take snapshot 📷.Testing Connections to ElasticsearchNow let’s what all of these headache got us. Execute the command curl --get https://YOUR_OWN_IP:9200.Testing ConnectionsNotice that it failed for the first time? Yeah, like I said, if it’s too good to be true in IT, chances are you missing something. Apparently the reason for the failed testing is because while the Kibana and Elasticsearch trust each other, the browser is don’t.Kinda like when you bring your other friend to your other friend, of course they still don’t trust each other instantly right?. That is why as their friend you would say “Trust me bro, this kid is good” and that is exactly what we would do with the command curl --get https://10.0.2.15:9200 --insecure | jq the --insecure is the “trust me bro” in this case where we bypassing it temporarily and as for the jq telling the output to make on JSON format. This why we install JSON packages previously.Creating CredentialsNow one more thing before we see ELK stack frontend, we have to generate credential in order to login to ELK stack. Here’s the command sudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto and basically this means we let the Elastic generate it for us. All the information about it can be found here. Here’s what it will look likeGenerated CredentialsAnd don’t forget to save all of that credentials, oh and also don’t change the elastic.username but just change password of it to the generated password previously. The ELK stack will stuck on endless loop if you don’t change those two properties. The elastic username used for superuser and it basically the core user of ELK. Changing it will make ELK stack error.Now with all of that restart the ELK stack with sudo systemctl restart kibana &amp;&amp; sudo systemctl restart elasticsearch.Now we ready to login ELK stack and using it. But before that, don’t forget to take snapshot 📷.Rule Forwarding For ELKOne last setting that we need to set before connecting to ELK stack is to add another port forwarding rules but this time for the connection to ELK stack. For the sake of simplicity, here’s the step below.Adding ELK Connection In Port Forwarding RuleTo add the rule above, we head to Virtual Box Manager just like previously we add SSH rule and then select Network &gt; NAT Network &gt; select Port Forwarding tab &gt; and click on the green plus icon. Just like that, we add the rule just like shown picture above. Finally, with this we could connecting to the ELK stack.Moment of TruthOh and if you just start again don’t forget to start the ELK stacks. I’ll explain it further but take a look this screenshot.Let The Browser Trust The ConnectionThe browser showing that because it still don’t trust the ELK stack connection yet and warning us. But we know this is a trusted connection and let the browser know that by clicking Advanced… and Accept the Risk and ContinueKibana Server Not Ready YetInteresting that we access it with https://127.0.0.1:5601 but not with YOUR_OWN_IP. Why is that? Well, that is because the ELK stack hosted on your own IP but not accessible publicly, that is why accessing it locally with 127.0.0.1. And what’s that not ready yet message? Welp, just like the name suggested, the system is indeed not ready yet, maybe warm up or smth ¯/_(ツ)_/¯.We a bit and….. voilà, the ELK stack front ladies and gentlemen. Phew… that was long journey huh? Nicely done guys.ELK Stack Finally Able To UseWhat’s About the Wrong IP Before?This last section is to explanation the silliness that made previously by entering wrong IP. Basically what will happen is that ELK stack will infinitely on Not Ready Yet status and to fix it?Infinite Kibana Server Not Ready YetTo fix it we have to change it back to your correct IP in kibana.yml like shown in the picture below.Change that 10.0.2.15 With Your Own IpAnd that 10.0.2.15 needed to be change to your own IP. With that, you finally fixing the Not Ready Yet status.Last Words and ReferencesThank you so much for spending your time following and reading through all of the yapping from me. I really hope this guide helping you on some way with your learning journey of cyber security stuff. And with this final words, I want to reference some of the sources that I used in this part of guide.            References                  nubb guide on github. Elastic-SIEM-Setup              LevelEffect guide by Will Nissler. How to Set Up Your Own Home Lab with ELK              ELK stack definition. elastic              Elastic discovery documentation. Discovery and cluster formation settings              X.509 definition. X.509              PrintNightmare information. Demystifying The PrintNightmare Vulnerability              SSL and other security information of ELK stack documentation. Security settings in Elasticsearch              PEM certificate formats information. Privacy-Enhanced Email              Kibana endless not ready yet status fix. Kibana server is not ready yet [closed]      "
  },
  
  {
    "title": "Detecting Windows Logon Events 4624-4625 With Elasticsearch As a SIEM",
    "url": "/posts/detecting-windows-logon-vents-4624-4625-with-elasticsearch-as-a-lightweight-self-managed-siem/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, windows, security events, detection",
    "date": "2025-04-12 13:40:00 +0700",
    





    
    "snippet": "Detecting My First EventFirst time I’m diving into cyber security and being tasked to explore Elasticsearch and it’s functionality, that is where I’m really in a dark as to how and where to begin w...",
    "content": "Detecting My First EventFirst time I’m diving into cyber security and being tasked to explore Elasticsearch and it’s functionality, that is where I’m really in a dark as to how and where to begin with this journey ahead. But then I watch some videos on youtube and in it Elasticsearch can be used for events detection. At that time, there’s a light bulb lights up on my mind because I thought to myself “How about I simulate a basic events using this local Elasticsearch that I’ve just installed and see what is it looks like in the Kibana? And along the way I could also learn other cyber security concept. Two birds with one stone” I said to myself.My Thought Process Looks like This Fr FrSo with this post I’ll explain and walk through about what is my setup for this scenario, the configuration for the setup, what kind of queires for logs that I used, what I learn from this little experiment, and some bits about what I should know more. So if you feels like just like me, new to Elasticsearch as SIEM or maybe just cyber security in general especially blue team, I hope this experience of mine helps you.What Is Event ID 4624 &amp; 4625? And Why Need To Detect Them?So for starter, all of the processes and events that is happening inside the computer are being recorded and stored in a form of logs that being specified by the system’s audit policy. And that is called Windows Security Logs, where it’s offer an immense amount of insight into what’s happening on a computer/machine. Our so called ‘main highlight’ for today is event ID 4624 &amp; 4625.But before we delve more in event ID 4624 &amp; 4625, let me explain what exactly does ‘ID’ means here. So in term of windows event logging, ‘ID’ is referring to unique identifier that uniquely identifies particular events that help define it’s description so that viewers can present these description to the user and thus helping user solving their own problems.And with that said definition we could finally explain and understand better what are those two events exactly.  ✅ Event ID 4624 – Successful Logon          Generated when user logs on successfully to a computer/machine.      This is very Useful for understanding who logged in, from where, and using what method.        ❌ Event ID 4625 – Failed Logon          Generated when a logon attempt fails by user.      Extremely useful for detecting brute force attacks, incorrect password attempts, or even internal misuse.      And with that information and adding documentation from Microsoft’s official documentation and the event ID reference, we could at least expect the information would be provided with fields like:      TargetUserName: Who tried to log in        IpAddress: From where        LogonType: What method (RDP, Interactive, Network, etc.)        FailureReason: For 4625, why the login failed        ProcessName: What app triggered the login  As we can see, the information really essential for us to help spot some patterns, suspicious behaviour or unsual acitivity in general.The Necessary ToolsFor the stack that I used in this little project to gather, store, and visualize the logs being gathered are as follows:      Winlogbeat (Agent installed on Windows host)        Elasticsearch (Storage &amp; querying engine)        Kibana (Dashboard for data exploration)        Windows Installed Device(Generating real login events)  This stack is a part of the Elastic Stack (ELK), which is widely used for log management and lightweight SIEM purposes.Step-by-Step Setup  ✍️ Step 1: Install Winlogbeat on the Windows HostDownload and install Winlogbeat from the official Elastic downloads page.Edit the configuration file (winlogbeat.yml):winlogbeat.event_logs:  - name: Security    event_id: 4624, 4625Specify the destination for Elasticsearch:output.elasticsearch:  hosts: [\"http://localhost:9200\"]Enable the Winlogbeat service:.\\winlogbeat.exe install-serviceStart-Service winlogbeatNow that the winlog is set, we then can proceed to next step. Connecting the winlog to Elasticsearch and Kibana.      🔗 Step 2: Connect Elasticsearch and Kibana                  Ensure Elasticsearch is up and running: http://localhost:9200                    Open Kibana: http://localhost:5601                    Navigate to Stack Management &gt; Index Patterns and create a pattern like winlogbeat-*            Start Querying Those Windows Logon Events in KibanaWith all of the setup being done and Kibana is ready to use, head to Discover menu on the Kibana like in the picture below.Kibana Discover Working Properly Showing Logs &amp; EventsAfter inside the Kibana Discover tab, we could see all of what’s happening inside our machine/computer. This will give us a clear information to pin point the threats, adverseries, and any other suspicious activies.The next steps to see the wanted events is to input the queries. We can input the queries like shown in the picture below alongside the necessary queries.Successfully Filtering Event ID 4624The language being used in the picture above is called KQL(Kibana Query Language) but before we continue with the event. Let me briefly expalain what is KQL in my understanding.  🧠 So What is KQL (Kibana Query Language)?It’s a set of search syntax in Kibana to filter and explore log data within Kibana that is designed to be intuitive and user-friendly for analysts to be able build queries without writing complex raw Elasticsearch DSL.They way it being structured is by having field-based searches, logical operators, and even wildcards to filter events quickly. Here’s the composition of it explained in table form:            Component      Example      Description                  Field match      event.code: \"4624\"      Match entries where event.code is exactly 4624.              AND / OR operators      event.code: \"4625\" AND winlog.event_data.LogonType: \"10\"      Combines multiple conditions. AND = both must match. OR = either one.              Grouping values      event.code: (\"4624\" or \"4625\")      Matches either value in a field.              Wildcard search      TargetUserName: \"admin*\"      Matches values that start with admin (e.g., administrator, admin1).              Phrase match      ProcessName: \"C:\\\\Windows\\\\System32\\\\svchost.exe\"      Searches for exact phrases, often used with file paths.              Negation      NOT TargetUserName: \"SYSTEM\"      Excludes logs where username is SYSTEM.              Time range      @timestamp &gt;= \"now-1d/d\"      Shows only logs from the last day. Supports relative time filtering.      How To Simulate Event ID 4625With the knowledge of how KQL structured and compose we can now then simulate how to make the event appear. Because event 4624 is a success logon, for this section I try to emulate event 4625. This is because currently I’m using one machine/computer only.There’s various way to emulate it, here are some of it:  Using Runas with Wrong Password          Open CMD or PowerShell      Run the following command on it runas /user:fakeuser cmd      When prompted, enter any password (wrong is fine).        Create a Dummy Local Account, Then Log in WrongIf it need to be more realism:                  Create a dummy user using PowerShell(run as administrators):net user admin P@ssword123 /add            Now try logging in with this account using the wrong password:                  Lock screen (Win + L) → switch user → admin                    Try logging in with wrong credentials:                  Use something like admin123 as the username and any wrong password.                    Delete the account after testing:                  net user admin /delete                     What Is It Looks Like Inside Discover in Kibana?And with that being done, we then can proceed to monitor the activity using some of the queries. Here’s some explame of it:  All successful logins    event.code: \"4624\"        Failed logins via local    event.code: \"4625\" AND winlog.event_data.LogonType: \"2\"        Filter out noise from system accounts    NOT TargetUserName: \"SYSTEM\" AND event.code: \"4625\"        Failed login attempts where the username starts with “admin”    user.name : \"admin\" AND event.code: \"4625\"        Failed logins from the last hour    @timestamp &gt;= \"now-1h/h\" AND event.code: \"4625\"      And that’s some queries that I tried to shows the regarding events on this little experiment of mine.Insights &amp; Lessons LearnedHere’s what I learned while building this setup:      Elasticsearch is flexible: You can query just about anything if you understand the schema.        Fields matter: Understanding LogonType, TargetUserName, and ProcessName can surface important patterns.        Kibana makes learning fun: Seeing the data visually helped me internalize the concepts faster.        Winlogbeat is efficient: It shipped the logs reliably with minimal config and resource usage.        Also: filtering out logon attempts from Anonymous users or from NT AUTHORITY\\SYSTEM helped clean up the data.  What I Still Don’t Know (Yet)This little experiment raised more questions that I’d like to explore:      How can I set up alerts for excessive 4625s in a short time? (e.g., brute force detection)        What is the best way to enrich logs with user info or location?        Can I use Elastic Security (SIEM app in Kibana) to build detection rules easily?        What about other logon-related event IDs like 4768 (Kerberos ticket request) or 4771 (pre-auth failure)?  Conclusion: My First Step into Detection EngineeringThis post helped me move from theoretical understanding to practical skill-building with Elasticsearch and Windows logs. I now better understand:      How login events work on Windows        How to query and filter them using KQL        How to use Elasticsearch and Kibana as lightweight SIEM components        Next, I want to explore Elastic Security, and maybe even try shipping Linux logs or network traffic into the stack. The possibilities are wide open.  If you’re on the same journey or have tips for me, I’d love to hear them. What logs or events do you think I should dive into next?Thanks for reading! If you found this helpful, check out my [first post] where I talk about the installation pains and how I made sense of the Elastic ecosystem as a beginner."
  },
  
  {
    "title": "Elastic Self-managed Installation",
    "url": "/posts/elastic-self-managed-installation/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, blog, itsecurity",
    "date": "2025-04-11 16:25:00 +0700",
    





    
    "snippet": "What Makes Me StartI just started my first work as SOC Analyst back in march 2025. When I first started, my manager asking me to install and play around with Elasticsearch. An application that I ne...",
    "content": "What Makes Me StartI just started my first work as SOC Analyst back in march 2025. When I first started, my manager asking me to install and play around with Elasticsearch. An application that I never heard before. My first thought is just “meh, it’s just another apps, what could be so difficult about it”.Basically What My Experience Looks likeOh boy was I so wrong. Elastic is a giant to learn in itself, so much of features, processes and many other that I should learn from the scratch. So in this post, I would like to begin with how I setting up my Elasticsearch. Hope you guys learn something from this post.What is Elasticsearch exactly?So, what’s about this ‘Elasticsearch’ being so difficult to me when I first encounter it, is that there’s three in one shenanigans for this applications. Imagine your system is screaming — “Things are breaking!” — but all the signs are scattered across thousands of log files in weird folders. You don’t have time to read all of them. Elasticsearch is like a giant smart search engine that stores and analyzes those logs fast. It helps you make sense of what’s happening in your system.So here’s a diagram to visualize how Elasticsearch worksDiagram of How Elasticsearch WorksAs you can see on the diagram, there’s so many components accompanying Elasticsearch itself. And from my experience alone here’s some of steps or things that I learn during my trial and errors.What I learned:  Always follow the correct order of installation: Elasticsearch → Kibana → Fleet.  Keep your tokens and fingerprints handy, don’t forget them.  Watch logs closely during setup — they often explain what’s wrong.The Installation ProceduresSo with that in mind, the steps that I follow to installing Elasticsearch and Kibana is this video below by Wasay Tech Tips.What’s Missing From The InstallationBut, here’s the catch. After I done following and installing Elasticsearch and Kibana based on that video, I still don’t have log or any data showing up on my Elasticsearch. And turns out, I need to install the Agent for it to be working properly.So the way Elasticsearch works is that the node or in this case my laptop to be able to sent log or data to Elasticsearch is by having agent installed on my device. So that is what I do, installing the agent and here’s how I do it.  First you need to install the agent from the official Elasticsearch website here.      After downloading it, extract the file and head inside the file of the downloaded agent file and then type cmd on the address bar and press enter. This will make cmd open on that file path.    The cmd shows up, don’t enter installation command yet. Head to your Elasticsearch localhost shown in the picture below.  In the Fleet menu, click on “Add Fleet Server”.  Next, the add server menu shows up and this is where we could select where we wanted to put our fleet server. In my case, I put on the localhost address with port 8220(you could get the localhost address from the configuration yml file) and click continue.  The fleet server location has been set, next step is to copy the installation command line to the previously started cmd. But keep in mind that for me, this command line from my localhost Elasticsearch is missing some key command line that I will show on the next step.  And for the final step of agent installation is to add this command to the cmd.    .\\elastic-agent.exe install ^  --fleet-server-es=https://192.168.56.1:9200 ^  --fleet-server-service-token=AAEAA... ^  --fleet-server-policy=6be1b061-868b..... ^  --fleet-server-es-ca-trusted-fingerprint=f54c4b...... ^  --fleet-server-port=8220        For the server service token, server policy, and ca trusted fingerprint it should be generated automatically if you on fresh installment. For my case, this is because I have previously installing agent and uninstall it again. So you should be getting it automatically if it’s firt time installment.Next, paste it on the cmd like picture below.  And with that, the Elasticsearch installation should be completed now.Setting Up IntegrationBut we’re not done yet, we still need those integration for our Elasticsearch to work properly. So next up is, setting the integration.  From the navigation side bar menu, we can find the Integration tab. Click that and we will be routed to integration menu.  Once we in the integration menu, we could then search for the relevant integration that satisfy our needs. For this tutorial, I need Kibana for visualizing my log so that I could understand all of the data that being collected. We could use the search bar for finding Kibana. And once Kibana found, click on it.  Now that we’re in Kibana installation menu, click “Add Kibana” button. This way we can begin the installation process.  Usually, all the previous steps above should be set by default. The one that we want to set however is “Where to add this integration” option. This option asking us in what agent policies that we want to put out integration. For me because my fleet server is on agent policies 8, I have to put my Kibana on that so it can work with the fleet server.  After that, it will give prompt like this. Just click “Save and deploy changes”.  And wait for the installation to completed and voilà our integration installed successfully.But What Agent Policies?Now you might be wondering, “What is this policies exactly? Are they like rules? Or what?”. Now when I first installed Elastic I also have the same question and turns out, Agent Policies are like a set of instructions that tells each Elastic Agent what to do, what to collect, and where to send it. It’s like handing your agent a to-do list:  What logs or metrics to monitor (e.g., Windows Event Logs, system metrics)  Which integrations are enabled (like Winlogbeat, Filebeat, System, etc.)  Where the data goes (usually to Elasticsearch)  Whether the agent can auto-update or be centrally managedEach device running an Elastic Agent is tied to one of these policies. So if I have 5 computers with Elastic Agents and they’re all doing the same job (like collecting Windows logs), I can put them under the same policy. That way, if I change something in that policy — boom 💥 — it applies to all of them at once. Super convenient.Elastic even lets you have multiple policies so you can customize agents differently for servers, workstations, cloud VMs, etc.The DashboardAnd with all of the installation process, here’s what Elasticsearch dashboard look like if the installation succeeded.Final ThoughtsThis blog isn’t meant to be perfect. It’s real. If you’re learning Elastic and security monitoring with just one machine and zero prior experience, I hope this helps. My goals for writing this blog is to help others and future me in-case need to start from zero again on the elastic installation, so this blog could make as an knowledge based to others and me too.Thank you so much for reading this and have a nice day where ever you are."
  }
  
]

