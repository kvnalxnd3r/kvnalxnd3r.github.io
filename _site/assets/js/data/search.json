[
  
  {
    "title": "Brute Force Attack Using RDP",
    "url": "/posts/brute-force-attack-using-rdp/",
    "categories": "Attack Simulation, Brute Force",
    "tags": "elasticsearch, siem, windows, rdp, kali, brute force, cyber attack, endpoint, logs",
    "date": "2025-05-29 12:33:00 +0700",
    





    
    "snippet": "About This PostThis post made because I thought “Okay, I got my tasked by my workplace to learn how to make Detection Rules and Active Directory. Maybe I could search something on the internet that...",
    "content": "About This PostThis post made because I thought “Okay, I got my tasked by my workplace to learn how to make Detection Rules and Active Directory. Maybe I could search something on the internet that connect these two?” And to my surprise there’s already someone that did what exactly I thought. Shout out to MyDFIR for the video that I used as my main basis and reference throughout this post.So the project that I will do will mostly following along the MyDFIR tutorial that consist of five parts that you could check here. You could also check his playlist on your own to see what kind of project that suit your need. But for me I’ll just scroll down on that playlist and start on his Active Directory project.MyDFIR Active Directory PlaylistHis project is also serve for my further learning about Brute Force, RDP, Kali Linux and its tools and many more new things for me. So with all of these background being said, I’ll continue to what context you should expect from this post.Context Before We StartThis section is to explain to you guys what you should expect from this post and that is the objective, purpose, and some disclaimer. Okay firstly the objective of this post is to simulating an attacker brute-forcing using RDP to the target machine (Windows). And with the attack being launched, we want to simulate SIEM Detection of it using Elastic while also documenting the detection rule creation, what challenges when making this rule, and some basic case handling.Second, the purpose for this post is to journalizing my learning so that maybe others could learn one or two things from this post. Even if you don’t learn something, well at least I hope you could be entertain seeing me being a complete noob. Lastly, disclaimer. This post by no means is perfect, so take it with a grain of salt and always check whether what I yapped is a mistake or not.With all of that out of the way, lets start the learning.What We Will Be UsingThe machines and tools and that we will be using is consisting of:  Attacker machine: Kali Linux  Target machine: Windows 10 Pro with Active Domain and RDP enabled  SIEM: ELK Stack  Brute force: Hydra  Remote desktop: xfreedp  Log forwarder: Winlog and SysmonThe ingredients is different from what MyDFIR using but the concept and the goals basically the same. So you could also adjusting that list to your need.Preparing Lab DiagramSo after we know what is the tools and machine that we will be using, we need to prepare it first before we start with our project. Firstly we need to have a clear diagram of this whole project. Think of it like a map or instruction that will makes us understand how to navigate and operating this project to meet our need.Project DiagramThis diagram, I make it using draw.io and from it you could see that I plan to use 4 machines in total, Kali Linux(for attacker), Windows 10 Pro and Windows Server 2022 Pro(I’ll tell you the reason on the next section), Ubuntu Server Linux(for ELK stack server). For the IP itself, I’m using DHCP from prefix of my already set NAT network, why DHCP? Because I mostly see people on production stage use DHCP(but half of the reason is that I’m lazy to think of network).Why we need internet here? Aren’t we running it locally? Well yes, it still running locally but the internet is used for downloading for updating dependencies, downloading and communication between machines. The diagram now done, next we preparing the machines.Story Time And Kill Chain DiagramLet’s add story for this project for the funni, so two weeks ago, a junior IT admin proposing a resignation after being denied a promotion. But day before his official resigning, the SIEM begins logging dozens of failed RDP attempts from an internal IP toward the legacy Windows 10 remote access machine still exposed via port 3389. After several hours, a login is successful — and lateral movement attempts begin. No ransomware deployed… yet.Was this a random brute-force? A targeted attack? An insider handing off credentials? The SOC must act fast — reviewing RDP logs, building custom detection rules, and hunting traces before the attacker escalates privileges or exfiltrate data.To further our understanding of this attack scenario, here’s the kill chain diagram based on LOCKHEED MARTIN cyber kill chain.Kill Chain DiagramAs you can see, the final objective for this lab project is to make adversaries gaining access for our data, so we need to act accordingly to this. Let’s start the lab progress shall we?.Installing The VMs  Remember on the lab diagram section I told that you need to use Windows Pro edition for all of the Windows VMs? That is because the Pro version enabling us to use RDP but the other Windows version won’t. Don’t ask me why but that is just how Microsoft is.For this section I could refer you to follow my two parts blog, part 1 part 2 to preparing our SIEM server and one of the endpoint/target machine. As for Kali Linux installation, you could follow MyDFIR part 2 video at minute 8:58, lastly for the Windows server VM at minute 12:31.  Don’t forget to add NAT networks and port forwarding rules for our VMs able to communicate each other before configuring Windows server.Well, apologize if you felt that I’m so lazy for just referring to that video(which is true) but mostly I felt that there’s no need to ‘reinventing wheel’. And after the installation complete, next we preparing the Windows Server which is on part 4 just follow along his video but for the static IP configuration part just skip it because we using DHCP other than that just follow along the video and we will good to go for our Windows server.  Oh while we’re at it don’t forget to also add Elastic agent and Sysmon on Windows server VM too. The how to do it, you can see it on my part 2 post. This optional but I feel the need to do this to simulate real life scenario.We skipping part 3 of MyDFIR video because he using Splunk and we don’t do that here but if you wish to use Wazuh or any other SIEM feel free to use it. The basic remains the same. And with all of it done, we can move to the fun part part, attacking the Endpoint.Adversaries Commencing The AttackNow at this point we have all the VMs and scenario set in stone, what is left for us is to commence the attacking scenario. And the first phase of the attack is the Recon phase.ReconOn the kill chain diagram it stated that the attacker start by scanning the network IP. How’s the attacker the network IP of that said active directory? It’s suspected there’s an insider informant for the attacker and the fact that the adversaries already inside the internal network really strengthening the suspicious insider among us(I’m sorry for that reference, but I had to).Because the attacker already infiltrating the network and got information of the network IP that being for the active directory, he/she then begin to scan that said IP. Here’s what scanning the IPs would look like from the attacker perspective.Attacker Scanning IP NetworkAttacker Got List of Available IPAs you can see, the tool that being used is netdiscover. This tools allowing the attacker to got clear vision of what is inside the network. The next step is for the attacker to scan what is the port that available on those IPs.Attacker Scanning For Open PortsNow the attacker start scanning what port that available on that IP range using nmap and would you look at that, he already got got so much information. The attacker knows the network got port 3389, 139, 445, 135, and 22. A quick google search will tell you that:  Port 3389: used for remote connection(RDP).  Port 139: used for file sharing, printers, and network for Windows-based system over NetBIOS, basically for older version for Windows version or Unix version.  Port 445: used for file sharing, printers, and network over TCP/IP and more of a modern version.  Port 135: used for allowing clients and server communicate to remote access and management. Or in other word, implement RPC Mapper Service in Windows environment.  Port 22: used for SSH, a secure remote login and command execution.As you can see, so much information gained by the attacker, and by now the attacker knows what IP to attack and that is 192.168.68.2 and 192.168.68.7 because the RDP port is available on it. So now the attacker resume to the next phase.WeaponizationThis is where this blog and part 5 video of MyDFIR differ, if MyDFIR using template word lists while I will be using my own word list because the scenario here the adversaries got information about what is the username and password but not know exactly what is the password. That is why the attacker prepping a list of possible password from the clue given.So here’s how the attacker will be preparing the word list.Attacker Make Password Guess ListAttacker Inserting Possible Password GuessFirst, attacker will make password list consisting possible password guess and combination. Attacker knows this because receiving information that one of the account on the active directory domain is username mrizky and the password is something along mrizkyad abbreviation of mrizky active directory. The attacker also knows that the password is using capital letter and number combination that is why you see the password list having bunch of numbers and capital letter. As for the number of password possibilities, the attacker using 20+ possibilities of guess.Now the attacker ready for the next phase of it’s attack and that is brute forcing their way inside the target machine.Delivery And ExploitationI just combine this two part because the progress is continuous of each other, what I mean by that is we will be using Hydra to brute force and guessing the password from the list and Hydra will shows us what is the correct answer. In MyDFIR video he’s using Crowbar but for me I failed when using it and find success when using Hydra. So lets start the brute force.We know that there’s two target here, 192.168.68.2 and 192.168.68.7 so we will be attacking those two. Starting with 2.Attacking 192.168.68.2Attack that launched to 192.168.68.2 using command sudo hydra -l mrizky -P password.txt -t 4 rdp://192.168.68.2is failed, this caused by either the firewall blocking the inbound traffic, RDP is not enabled, or anything regarding security policy inside this machine. The command consisting of:  -l flag, this flag telling hydra to use username that given to it specifically.  -P flag, this flag telling hydra to use list in form of a file in this case its password.txt.  -t flag, this flag telling hydra to tried four times for each attempt of brute force.  rdp://IP_ADDRESS flag, this flag telling hydra to use RDP targeted to the IP being given.More about available flag of Hydra can be read on official page of it.Next we’ll attack 192.168.68.7.Attacking 192.168.68.7Now it’s a success! Let me attempt to explain a bit what happen here in this succeed attempt. The line where [3389][rdp]…… is where Hydra attacking the port that attached on that IP using one of the password that given in the list mrizky4d. And it failed but the next line it says the connection is failed right? That is either because the connection is bad or RDP rejecting another connection attempt, RDP is really sensitive about this by default.And Because the Hydra on the background running through the list checking all of the possible password is why the next line we got success notification in colored highlight. The password is mR1zKyaD, this is why I combined delivery and exploitation because the it can be executed simultaneously. Next let’s see the attacker using xfreedp to remotely entering the machine.InstallationAll the important information is acquired, next that left is for attacker entering the machine and let’s see how it is done.RDP Connection EstablishedFrom the picture you can see the attacker using xfreerdp3 tool and the command is sudo xfreerdp3 /v:192.168.68.7 /u:mrizky /p:mR1zKyaD.  /v flag is signalling to xfreerdp3 to attempting connection to the given IP.  /u flag is signalling to xfreerdp3 to use username being given.  /p flag is signalling to xfreerdp3 to use password being given.And from the look of it, the attacker now successfully connected and being given prompt to end other user that currently logged in as you can see from the picture below.RDP Connection EstablishedFrom the picture let’s just assume that whoever currently using the machine thinking that maybe this is from IT department wanted to log in, so he/she just accepting the connection. When the current user accepting it, here’s what it looked like on the attacker side.Attacker Connected To The MachineAfter the connection is established, the attacker finally inside the machine.Attacker Inside of The MachineAt this stage, the victim is at attacker mercy, because from this point on attacker successfully infiltrating the system and can do whatever possible.C2 With Actions On ObjectivesAgain, I combining this 2 phases because to my understanding this two phases as I practice it, it correlate with each other. Why? Because when attacker dropping the backdoor it also mean that accessing data is available and any other feature inside the machine. Let us see from the attacker perspective.Attacker Dropping Backdoor ScriptThe attacker making bat file in the C:\\Users\\&lt;victim&gt;\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Startup with script powershell -windowstyle hidden -command \"Start-Sleep 5; Start-Process calc\". The script basically will automatically open calculator when user mrizky login to the machine.  This script will only running when user mrizky logged in to the machine. If you use other user it will not running.The Script Executed When LoginYou guys see the script got executed right? This will opening calculator application and I try to simulate the attacker attempting another login using xfreerdp and the script successfully executed. This is just one simple example of an attacker planting some script. The script I shows in this project is just a harmless one that opening calculator(this is to match my skill that is just a noob in cyber security field) but imagine if the attacker is more skilled? How destructive could it be eh?Final Words About The AttackWith it the attack is done now. Quite fantastic and mind blowing I’d say to be able done this project because with this project I could picturing how is the attacker mindset actually got in action. But how about from the defender side?(Blue Team) I’ll show you guys how’s from the blue team side would look like.Defender Defending The SystemFrom the defender side we will see how them preventing, securing, monitoring, and maintaining the system from the attacker. From my understanding I would like to show you guys how can defender preventing and defending from some of the key phase from the attacker so that we could securing the the system. Without further ado, let’s get into it.As I’m using Elastic as SIEM, we will take a look how Elastic work, and better way to showcasing it by showing the dashboard that using queries to will help us taking information about event, logs, and other process on our endpoint machines. Here’s some of the queries that will help us achieving that.      event.category : \"network\" and destination.port : * and network.transport : \"tcp\" for querying TCP activities. This query would help us scanning the activities of network that using TCP protocol. Why TCP? Because from nmap documentation they usually using TCP for their traffic, so I my guess is that we will focusing on the TCP. I also basing this query that I make from this rule from Elastic.TCP Connection Activity        event.code : 4625 AND winlog.event_data.LogonType : (\"3\" or \"10\") for querying failed remote connection attempt. I make this query because from the Windows official documentation about log on type 4625, it is a failed log and the type for remote connection is 3 and 10. So I just using the type with operator OR so that it could catch both or one of it.Failed Remote Login Attempt        event.code : 4624 AND winlog.event_data.LogonType: (\"3\" or \"10) for querying successful remote connection. From the documentation other than different ID number and this time its successful login, it’s basically just the same with failed log on attempt query.Successful Remote Connection        file.extension : (\"bat\" OR \"ps1\" OR \"lnk\") for querying script powershell execution. For this query, I just go simple and straight specifying the type of the file. I mean it’s not everyday someone would executing bat file or ps1(PowerShell) file right? So we could just heavily suspecting it to be suspicious activities.Executed PowerShell Script  With all of those query that I could think of, this is some of the steps that I come up with as Junior SOC Analyst would do, is to first as you can see testing whether the query is getting hit and from all of the picture above it is proven we got hit. So next thing to do for me is to make Rules for those queries.We go to Security tab inside the hamburger menu, next click on Alerts menu and then go to Rules page. We then would arrive in the dashboard of Rules where we can create the rules that we wanted.Rules Dashboard PageFrom that dashboard you can see that I already make one of the rules, but don’t worry I’ll show you how to do it and first click on Create new rule, we will then presented the options and setting for us to make the rules. I will show you how to make first rules, start with the prevention of Reconnaissance phase of the attacker and that is for detecting network and port scanning attempt based on the information that we got. Just follow along the picture that I show you guys below.      First it will asking you guys to inserting the query, just insert it like this picture.Rules Query Inserted        Next, we will filling in information about what this rule would help us detecting attack type based on MITRE ATT&amp;CK framework. For this I’m referring this rule with Discovery technique attack and also Reconnaissance technique attack because when I read the definition, the description matches with how the attack being executed and operated.  To make this rule more specific I also specifying the sub-technique attack of it to Network Service Scanning/Network Service Discovery, Active Scanning - Scanning IP Blocks.  For the severity level I set it to low because sometimes this rule detecting false positive and to be honest, I’ll learn more about queries in the near future but for my current skill this is what I could think of.Specifying Rule Information      We already got the information, now what we wanted is to set the schedule of how often this rule will be applied and running. For this I highly suggest just got with whatever ideal for you and for me it will be running every 20m with additional check between 1h.Defining Rule Schedule        Final step is to specify what kind of alert application or method that we want to set this rule for help notifying team about our finding using this rule. For me, because I’m in lab environment I’ll just go with no actions. But in real life scenario you could choose from Microsoft Teams, Slack, etc.Rule Action Notification  And that’s it, the rule has been created, next thing left is to make rules for the rest queries. I’ll just showcasing the important part of it because it’s basically same procedures as our first rule.      For detecting brute force attempt rule.Rule Queries Brute Force AttemptRule Information Brute Force Attempt        For detecting external remote access rule.Rule Queries External Remote ConnectionRules Information External Remote Connection        For shell script execution rule.Rule Queries Shell Script ExecutionRule Information Shell Script Execution  And with that, we got ourselves custom rule that matches our condition and need. Let’s see if this rule working as intended or not. To see the result, just click on one of the rule that we make previously in rule dashboard page and Scroll down a bit to see if there’s any result we get. I’ll start with brute force attempt rule because for me this where the attacker start being aggressive.Brute Force Attempt Rule ResultTo see the result just click on View details icon and we will be presented with this menu on the right side of the screen.Taking Action For Rule ResultJust click on the Take action button and select Add to new case. Next is shown on picture below.Filling New Case InformationMaking New CaseAfter we click add new case, we will then have to inserting the case name and description to better explain what is wrong with this case. Next we will see whether the case has been created or not. To do that, head to Cases menu page by clicking the tab on the left and we will see this pages.Cases ListOur new cases successfully added now we can follow its development as other team will be notified by clicking on it(External Remote Connection).Case ManagementBecause I’m just L1 SOC this is as far as I can do for my current skill and knowledge and with that we just have to wait other team confirming our finding.Final WordsThank you so much for following this blog and I hope you guys could learn one or two things from it but if it’s not then at very least I hope I could be entertaining enough with this. Once again, thank you so much and also thank you MyDFIR for inspiring me doing this project.References            References                  MyDFIR Playlist. MyDFIR Active Directory Playlist              LOCKHEED MARTIN Cyber Kill Chain. Cyber Kill Chain              Kali Netdiscover. Netdiscover              Kali Nmap. Nmap              Kali Hydra. Hydra              Kali Xfreerdp3. Xfreerdp3              Elastic Port Scan Rule. Potential Port Scan Detected              Windows Failed Log on Event. 4625(F): An account failed to log on.              MITRE ATT&amp;CK Discovery Technique. Discovery.              MITRE ATT&amp;CK Reconnaissance Technique. Reconnaissance.              MITRE ATT&amp;CK Network Service Discovery Technique. Network Service Discovery.              MITRE ATT&amp;CK Active Scanning Technique. Active Scanning.              MITRE ATT&amp;CK Scanning IP Blocks Technique.  Active Scanning: Scanning IP Blocks .              False Positive Definition. False Positive.      "
  },
  
  {
    "title": "Integrating Fleet Server And Preparing Windows VM",
    "url": "/posts/integrating-fleet-server-and-preparing-windows-vm/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, windows, fleet server, elastic agent, endpoint",
    "date": "2025-05-20 15:15:00 +0700",
    





    
    "snippet": "A Little ReminderThis is part two, continuation from my previous post(part one) where we walk through installing ELK stack SIEM. On this part two, we will take a look at how to integrating our ELK ...",
    "content": "A Little ReminderThis is part two, continuation from my previous post(part one) where we walk through installing ELK stack SIEM. On this part two, we will take a look at how to integrating our ELK stack with endpoint, in this case is a Windows VM.We will learn also what is Filebeat, Fleet Server, Elastic Agent, Elastic Integration, and many more tools for getting our lab work as a complete eco system for simulating SOC monitoring, log capturing, threat hunting, and maybe many more in the future. Without further a do, let’s get into it.Creating Administrator UserFor this section we will create administrator user so that instead using superuser account all the time. The reason being is for best practice about privilege(cmiiw). Alright, let’s just login to Kibana first using our superuser like in the screenshot below.Login To Kibana Using SuperuserWith superuser account, we could login to Kibana and start to make new user. In the screenshot shown below we will be greeted with Home page menu, don’t worry if it’s just showing template menu, that is just how it is for fresh installment. Now click on the Hamburger Menu on the left side and there will be more sub-menu. We need to click on Stack Management menu.Heading To Stack Management MenuAfter that, we will be directed to the Stack Management page. Here, there’s so many options for you to set but for now let’s just click on the Users sub-menu on the Security category.Click on Users Sub-menuNext there will be Users being shown. In here, listed various users, each with it’s own roles, status, and etc. Want we want to do here is click on Create user button on the upper right corner.Click on Create userWith that, we will then begin to inputting necessary properties for this planned administrator user. There’s various values and properties that you could play around if you want to set something in the future but for now we just focused on these properties in the screenshot below.Setting Up New Administrator UserWhat basically say to Kibana is that we want this newly created user to have username kibana-lab, password that we set, and the privileges such as beats-admin, kibana_admin, and superuser. From the looks of it, the selected roles is somewhat self-explanatory but if you want to know more of it, here’s the full documentation of it. And with that we could login to the new created user.Creating And Integrating Fleet ServerNow that we have new administrator user, we then could login onto it and start integrating Fleet server to our Ubuntu Server machine. This is where our Elastic agents being managed, more about fleet server could be found here.To add our new Fleet server just head to Management sub-menu inside the hamburger menu and click Fleet just like screenshot below.Fleet Server PageFleet server is need some time to load but once its done, click on Add Fleet Server button, it will then navigate us to this page where we could download Fleet Server installer.Screenshot below shows us that we first need to download the file, for next is to set the ELK stack to Production mode because we have our own certificates that we generated on previous tutorial. And on the fourth steps, we specified the location of where we want to attached our Fleet server host. In here, I insert the address of our Ubuntu Server machine and put it in port 8220 and then clickAdding Fleet ServerNow we go back to our Ubuntu Server VM, head to cd /etc/fleet and insert this command to download the Fleet server:sudo wget https://artifacts.elastic.co/downloads/beats/elastic-agent/elastic-agent-7.17.6-linux-x86_64.tar.gzDownloading Fleet Server/Elastic Agent Installer FiLeAfter done downloading it, we need to extract it using tar command:sudo tar -xvf ./elastic-agent-7.17.6-linux-x86_64.tar.gzExtracting Fleet Server/Elastic Agent InstallerAnd this will make directory named elastic-agent-7.17.6-linux-x86_64, but if you like me and somehow the files is on the Desktop directory of your linux unlike nubb tutorial. Then follow the screenshot below.Moving Fleet Server/Elastic Agent Installer FolderHere’s the steps that you could follows based on that screenshot  ls, I first checking on the files or folders my current directory. With it I could see there’s only 2 inside, which is just tarball of the installer file and the extracted file.  sudo mv elastic-agent-7.17.6-linux-x86_64/ /etc/fleet, is to move the extracted folder to /etc/fleet. Previously failed attempt happen because I don’t include sudo to elevate our privilege.  cd /etc/fleet/elastic-agent-7.17.6-linux-x86_64/, to navigate inside it.After we successfully navigating inside the installer folder, we then could insert these commands shown below.Generating Fleet Server Installation Command And TokenOn the screenshot we could see the command being:sudo ./elastic-agent install --url=https://YOUR_OWN_IP:8220 \\  --fleet-server-es=https://YOUR_OWN_IP:9200 \\  --fleet-server-service-token=YOUR_SERVICE_TOKEN \\  --fleet-server-policy=499b5aa7-d214-5b5d-838b-3cd76469844e \\  --certificate-authorities=\"/etc/fleet/certs/ca/ca,crt\" \\  --fleet-server-es-ca=\"/etc/elasticsearch/certs/elasticsearch.crt\" \\  --fleet-server-cert=\"/etc/fleet/certs/fleet.crt\" \\  --fleet-server-cert-key=\"/etc/fleet/certs/fleet.key\"Keep in mind, dont FORGET change the all of the IP’s to YOUR OWN IP. If we don’t do that, the server won’t be able communicate with the ELK stack. Here’s what it would look like on the terminal.Fleet Server/Elastic Agent Installation ProcessLast steps is to make sure the newly made Fleet server is able to communicate with the rest of our ELK stack. With that being set, go to Fleet page once again and click on the Fleet Settings on the upper right corner of the page. We then being showed this window.Setting Fleet Server AddressMake sure your setting is matched with your own IP, otherwise, the Fleet server will not showing any captured logs like this screenshot below.False IP Address Causing No Logs AppearingAnd lets check on the Fleet server page, it should be succeeded by now.Fleet Server/Elastic Agent Installed SuccessfullyAnd we finally able integrating the Fleet server with our Ubuntu Server machine.Creating Agent Policies And Adding IntegrationNow that the medium to gather and managed the all of the logs has been created, we need to create utilization in order for us to able collecting the logs right? And that is what Agent Policies are, they are a set of input that has been defined in order collecting data according to their configuration. The agents also worked with Filebeat to get specified logs and could be set working in tandem for log ingestion.Before we continue further let’s check on our Fleet server status and see what it’s look like.Fleet Server StatusThe status of our Fleet server is healthy, that itself is self explanatory but there’s also several status such as Unhealthy, Offline. Next we will be adding new policy.Now for us to be able making this policies we need to navigate by clicking Create Agent Policy tab on the Fleet page and then we will be shown this page.Fleet Agent Policies TabOn the image, we also seeing the default policies but what we need is our own custom policy and for that just click on the Create agent policy button for then will be showed Create agent policy window.Creating Agent PolicyI will be naming this policy Windows-Endpoint because we will be putting it on windows endpoint machine for collecting logs. And for the description, fills whatever suits your need. After all of that done, just click Create agent policy. After that the policy will be show up on the page now.And after the policy is available, click on it to configure of basically what integrations we will be putting inside of that policy.Selecting Newly Created PolicyNow we’re in the list page of all the integrations that is inside the Windows-Endpoint. In here we can see various integrations that available, right now only System integration that is available but we will add another integration. Just click on Add integration.Add IntegrationWhen we clicked on it, there will be Browse Integrations page and in here we can search also choose what integration that we would like to add. For us now, we need to search Endpoint Security for us to be able securing windows endpoint, so use the search button to search for it. Once we got the result, just click on it to set it up.Searching Endpoint Security EndpointNow we’re able to configure it to our need, I’ll name it windows-edr-integration, fill the description and lastly choosing previously made policy Windows-Endpoint.Endpoint Security IntegrationAfter all that done, just wait a moment and there will be success notification popped up.Successfully Adding Endpoint Security IntegrationThe window shows up, we will then click on the Add Elastic Agent later button and continue to adding also configuring another integration. The integration this time is Windows integration, that way it will help us collecting logs from our Windows machine. Just repeat the previous steps for adding it and then we will arrived in the setting window just like before.Configuring Windows IntegrationI just named it windows-log-collect for the sake of convenient and then choosing Windows-Endpoint policy. After that, continue to scroll down until you see Show advanced setting and click on it. We will then presented with various input column, we want to search for windows.advanced.elasticsearch.tls.ca_crt properties and in it we will paste C:\\Windows\\System32\\ca.crt path. This path will later be used when we copying the CA certificate from our linux VM to the Windows, but for now just set it. And we done for the Windows integration, just click Save and continue. We done with the needed integration, the integration should be ready to integrated to the target machine but because we don’t install any Windows VM yet, the next step is to install Windows VM.Preparing Windows VMHere comes another VM installation part, so let get this over quickly because it’s basically just the same stuff with the Linux installation.  First we want to download the ISO for Win10, to do that just go to multi-edition download site. Just follow along the instruction on the website.  Now that we downloaded media installation tool, run it and we will be shown window for us choosing what kind of action we want for the media creation tool. We want it to make ISO image for the Windows 10.Choosing Windows 10 Image Format  After the format has been chosen, we then select ISO file to be the media of our image. If you want it, you could also choose USB drive but for now we want it to be a file for our VirtualBox.Creating ISO Image  The EULA pop up, just scroll it until done and click Accept.Accepting EULA  Now we need to specify where we need to put the ISO file on our computer and don’t forget to naming it. With that done, the download then begin.Specifying File Location  When the download is done, head to VirtualBox and then click on Machine &gt; New and then select the newly downloaded Windows ISO image.  Don’t forget to uncheck the Unattended install and begin to set your system requirement, I’ll go with RAM: 4GB, CPUs: 2, Storage: 35 GB.  Now just following along the installation instruction but when you arrived at Which type of installation do you want? part, just choose the Custom: Install Windows only (advanced).  For the allocation of the partition, just use all of it but again, just go with whatever suit you if you want to play around with it.  And just wait for the installation to complete. After it finished, I suggest you disabled your internet momentarily. Why? Because the Windows will bothering you with all of its bullshit setup like Cortana(fucking hate this, it just bloat the Windows), especially they forcing you to make Windows account and we don’t want that. Disabling the internet just skipping all of that hassle. There’s other method tho like inputting Shift + F10 and oobe/bypassnro in the CMD. But for me it’s not working, so I just go with simplest solution.  And now, we done installing Windows VM, here’s what it look like.Windows VM  For me, we not done yet. There’s still one more thing to configure. And that is adding Guest Addition CD image. This adds makes us enable to fixing screen display(I don’t like the display), making us seamlessly changing screen(ALT+TAB), and many more. Just click Devices &gt; Insert Guest Additions CD image.Windows VM  After we done clicking that, there will be ISO image mounted on the Windows machine, just double click on it.Insert Addition Image  Inside the image, we will presented with various installer. Just double click on VBoxWindowsAddition.exe to begin the installation. Just for the installation to complete and after it complete, you will be asked to reboot the Windows VM. Just follow along.Mounting Addition Image  We then could check whether the addition image is installed or not by navigate to View and see if the Virtual Screen 1 has options for various screen resolution. And we done for the VM preparation.Windows Endpoint Preparation For Elastic AgentNow we can start by testing the connection of the Windows VM to the Ubuntu Server VM, where our ELK stack installed.We start by testing connection alongside the port whether it is listening or not. Just insert command Test-NetConnection -port INSERT_RESPECTIVE_PORTS YOUR_OWN_IP(remember to change it to your own IP and port that you want to test).Checking Connection To ELK StackNow we know the connection is succeeded, we then could proceed to installing Sysmon, basically a service for monitoring logs activity of our windows machine. We need to download it through PowerShell as Administrator and then insert these command:  Invoke-WebRequest -URI https://download.sysinternals.com/files/Sysmon.zip -OutFile \"C:\\Program Files\\Sysmon.zip\"  Expand-Archive \"C:\\Program Files\\Sysmon.zip\" -DestinationPath \"C:\\Program Files\\Sysmon\"Downloading SysmonBasically what all of those mean is us to download Sysmon, put it on C:\\Program Files, and extracting it. Now that we got the file, we could then cd \"C:\\Program Files\\Sysmon\" and insert the command:  Invoke-WebRequest -URI https://raw.githubusercontent.com/SwiftOnSecurity/sysmon-config/master/sysmonconfig-export.xml -OutFile \"C:\\Program Files\\Sysmon\\sysmonconfig-export.xml\"  ./sysmon.exe -accepteula -i sysmonconfig-export.xmlExecuting The Template ConfigurationBasically the commands says that we downloading template of Sysmon configuration by github user SwiftOnSecurity and here’s the link of the template.Now we have the template, we need a script to adjusting it, so just enter this Shell command:function Enable-PSScriptBlockLogging{$basePath = 'HKLM:\\Software\\Policies\\Microsoft\\Windows' +'\\PowerShell\\ScriptBlockLogging'if(-not (Test-Path $basePath)){$null = New-Item $basePath -Force}Set-ItemProperty $basePath -Name EnableScriptBlockLogging -Value \"1\"}The script basically checking whether if there’s registry key exist or not and if it’s not it will set the boolean condition to True with value 1.Enrolling Elastic AgentThis part where we start by transferring our CA certificate from Ubuntu Server to our Windows but first for extra measure, head to cd /usr/share/elasticsearch and copy ca.crt from there to home directory using sudo cp ca/ca.crt /. If the file is already there then it will showing notification that says there’s already same file but if it’s not then it will succeeded. Next thing to do, we just need enter this command inside the PowerShell as Administrator.scp YOUR_UBUNTU_USERNAME@YOUR_OWN_IP:/ca.crt ./ Copy CA Certificate From Ubuntu ServerThat command telling PowerShell to use secure file transfer protocol to copy a file from the Ubuntu Server VM. And now we will do the same with the within the Ubuntu Server that is to insert this command,  scp YOUR_UBUNTU_USERNAME@YOUR_OWN_IP:/ca.crt /usr/local/share/ca-certificates/  sudo update-ca-certificatesIt’s just copying file but this time we want to update our certificate in case it still uses old certificate.Copying CA Certificate For Linux Ubuntu ServerDon’t mind the failed first try command, that is happen because I was careless and not include sudo on my command.All that left is to download Elastic agent on our Windows VM:  Start-BitsTransfer -Source https://artifacts.elastic.co/downloads/beats/elastic-agent/elastic-agent-7.17.6-windows-x86_64.zip -Destination \"C:\\Program Files\\elastic-agent.zip\"  Expand-Archive \"C:\\Program Files\\elastic-agent.zip\" -DestinationPath \"C:\\Program Files\\elastic-agent\"Download Elastic Agent For WindowsThose two commands is just us telling Windows to download it and extracting Elastic agent file on the given directory. With that, Elastic agent installer is on our machine, then we would like to return to Fleet page in the Kibana, this time we will adding agent. The page would look like this.Adding Agent To WindowsThis page is inside Fleet &gt; Agent Policies Tab &gt; Windows-Endpoint(or your own named Windows endpoint policy). In here we click on Add agent on upper menu. After we click on it, there will be window shows up on our right screen that will instruct us on how to adding agent to our endpoint.Add Agent InstructionWe simply just generate our own enrollment token, also because we already downloading agent installer we just skip to next step that is installation command.Enrollment Installation ComamandWe inserting this command on our PowerShell, basically this command instructing the installer to installing Elastic agent with our address along with attached port, and it’s own enrollment token. And don’t forget to change to it’s directory!.  If your PowerShell location outside on any other directory, insert this command cd C:\\\"Program Files\"\\elastic-agent\\elastic-agent-7.17.6-windows-x86_64. You could replace the 7.17.6 with any other version that you installed.  .\\elastic-agent.exe install --url=https://YOUR_OWN_IP:8220 --enrollment-token=YOUR_OWN_TOKEN. You could also add another flag on that installation command --certificate-authorities=\"C:\\Windows\\System32\\ca.crt\", but for me I just go along with the command given by the Elastic.Enrollment Installation CommandAnd that’s it! Just wait a bit for the installation to finish itself and you got your endpoint integrated with Elastic agent policy. Let’s check on it, see if it’s really installed.Agent Installed SuccessfullyWe finally Done! What a long way huh, you could use this step to add another agent on different endpoint, adding more integration, making another policy and just explore on your own. I really hope this tutorial make you learn something. Before I finish this tutorial, let’s see what would our Kibana catching logs look like.Logs Appearing On KibanaAnother moment of truth, let’s what our progress done for us now. Click on the hamburger menu button, under Kibana menu section click Discover and now you will be shown dashboard.Captured LogsFinal WordsAnd we officially done with the installation series. From here on, you could start your own exploration, see what kind of scenario you want to set up or you could follow part 4 of LevelEffect by William Nissler. This guide is meant to teaching you guys how to catch the fish from my POV. You could also add your own twist but for me the basic stays the same.So again, thank you so much for reading and following this guide and hope you learning something.            References                  Elastic Documentation About Roles Built-in roles              LevelEffect Guide by Will Nissler Part 2. Home Lab: Enabling and Configuring Threat Intelligence and Detections              Fleet Server Definition. What is Fleet Server?              Elastic Agent Information. Elastic Agent policies              Sysmon Definition. Sysmon v15.15              SwiftOnSecurity GitHub. SwiftOnSecurity      "
  },
  
  {
    "title": "ELK Installation With Two VMs For Home Lab",
    "url": "/posts/elk-installation-with-two-vm/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, windows, linux, virtualbox",
    "date": "2025-05-20 13:21:00 +0700",
    





    
    "snippet": "Before We StartThis section is a explaination for what to expect in this blog and also serve as a shoutouts to all of the peoples that making knowledge in form of tutorials. With that being said, t...",
    "content": "Before We StartThis section is a explaination for what to expect in this blog and also serve as a shoutouts to all of the peoples that making knowledge in form of tutorials. With that being said, this is some of the people that helps me on this installation:  crin, thank you for making discord server where newbie like me and others could gather and share knowledge.  nubbieeee, thank you so much for reaching out to me on discord. Really appreciate your guide, with it  I’m able to present my progress.  Will Nissler from LevelEffect, thank you for making the tutorial that I could use as a main guideline alongside with nubb tutorial. Your tutorial really have a lot of information.  And other tutorial that I will list on the reference that helps me fix certain error and circumstances that I experience along my installation and setup progress.What To Expect From This GuideThis guide is served as a supplementary and a note for present and future me or maybe others that will or want to start installing Elastic for home lab where it could be used as tools of experimenting and hands on learning of cyber security. By no means this guide is perfect and if you happen to encounter any error, use the material that I listed on the refence section or like the old saying Just google it. Because at the end of the day, cyber security is heavily relied on the so called professional googling. And don’t forget to ask ChatGPT, it really helps to direct you to more accurate answer.I’m planning to make this guide into two parts or maybe more depending on my current assignment given to me. But for the time being it will be just two part. For this part one, I will focusing on installation and setting up the ELK stack and all of the VMs.Some Stuff That You Will LearnThere are some stuff here that I sure you will learn one or two when following this guide, some of it are:  Setting up and configuring VM with VirtualBox.  A bit of networking stuff such as port forwarding, networking.  ELK stack installation and understand how they work.And maybe many more that I couldn’t mention above. Hope this guide become useful for me in the future or for you guys too.Objective of This GuideOkay this will be the last I’ll yap other than the actual guide. So here’s some of the objective for part one of this guide series:  Setting up all of the VMs (i.e. Linux and Windows) to suit the requirement.  Setting up VMs connection configuration.  Installing and configure Elasticsearch on Ubuntu server VM.  Installing and configure Kibana on Ubuntu server VM.  Installing and configure Filebeat on Windows VM.  Successfully login to ELK stack after the installation.Let’s BeginNow in this section, we will begin the installation progress by first understanding what is ELK stack and then understand what this lab looks like on a diagram to further enhance our understanding of this lab. First thing first, what exactly is ELK stack? I’ll add my 2 cents here too since the official documentation already done such great detail in explaining it. Basically ELK is an acronym, Elasticsearch Logstash Kibana and each of that technology is serve their on purposes.Elasticsearch is to search, analyze, and process all of the data that being collected. You could call it the engine of ELK stack and Logstash is the instrument that ingest all of that data from multiple sources to Elasticsearch and when the data arrived in Elasticsearch, it is then can be visualize by Kibana for making user more easy to understand all of those data in form of charts and graphs.DiagramAnd with that explanation in mind, we then could utilize ELK stack as a SIEM to monitor our system. For the system of this lab, I will explain it with the diagram below to better understand the context of what it looks like:Here’s a simple diagram to illustrate our labI admit, it’s not fancy but for me it could convey what our lab will look like and this is as far as my understanding of the lab. But if you want a more detailed overview of the diagram, head to William Nissler guide he got to more detailed about it. So from that diagram, we want to install ELK stack on the ubuntu server with the listed ports and that port will be attached to the DHCP IP.And the ELK stack will receiving log mainly from the Windows VM through sysmon, filebeat, windows event log, and powershell script. And also the Windows VM having the IP received from the DHCP process that can be used to make connection between two VMs. Lastly, the Windows analysis machine is our own PC that act as a monitoring device that can access the ELK stack with the localhost ip. But why localhost you say? To put it simply, we can’t just access the VMs directly because the nature of VM itself.VM is run on their own separate kernel despite it’s still in our machine and that also mean they having their own network too, that is why in the diagram I put router to illustrate this process as router is the device that handling layer 3 i.e. network(IP, etc). And to access that network we have to set the NAT and some ports too if we have to use some protocol. We will go through this more in depth in the next section. But for now, just know that we can’t just access the VMs directly.Linux Ubuntu Server VM InstallationIn this section we will begin first with installing Ubuntu Server for our ELK stack. The Ubuntu Server version that I and some of the guides that I used is 18.04 LTS, the LevelEffect guide didn’t mention any reason as to why choosing this version but seeing the time of the blog being published was in 2022, my guess is version 18.04 is more stable than any version during that time.And if we done choosing the version, here’s some requirement for the Ubuntu VM:            Hardware Requirement                         OS      Ubuntu Server 18.04              RAM      4 GB              CPU      2 Cores              Storage      30 GB      I go with the requirement above because it suits my need and my hardware capability but if guys want to tweak some of the requirement then go ahead. With that being said, the steps for installing Ubuntu server is as follows:  Download it from the official Ubuntu server website.  After done downloading it, we then open VirtualBox and click Machine menu and select New….Selecting Machine To Create New VM  It will then having Create Virtual Machine pop up, with it we could configure and set the properties of the VM. Just put the machine hardware just like our hardware requirement previously. One thing to note here is that, I checked the Skip Unattended Installation because if we want to use unattended installation that mean VirtualBox will handles the installation based on the provided setting and in this case we want to configure it manually.Configuring VM Hardware  Next, the VM will appear on the VirtualBox menu manager, select that and click on Start icon to start the VM installation.  The VM starting and next thing is to select language, select whatever language that suits you but in this case I choose english for obvious reason.  The language set is done, now we will greeted by Ubuntu Server version notification. For our case, just select Continue without updatingSkipping Version Update Selection  Here, there will be IP that we got from DHCP, note it because this is important for basically every next configuration.Receiving IP DHCP  If being asked to choose proxy on the next step, just skip it because we don’t need it.  Also just go with the default mirror link. Basically this is just Ubuntu asking for sources to download its dependencies and packages.  The next menu is storage partition. For me personally I’ll just go with the default but feel free if you guys want to get creative with it.  Okay, next is configuring username and password for the Ubuntu. Remember also to make password that easy to remember but also not too easy to guess(it will be used a lot in the next steps). For this section, I’ll configure it like shown below but then again, feel free just go with something you guys want.Creating Account For Ubuntu  The account has made, we then continue to enabling SSH for this Ubuntu.Selecting Enable SSH  All of that has been done then we wait for the installation to finish. It will somewhat look like this or similar.Ubuntu Server Successfully InstalledNow with the installation complete I’ll add one more important thing to do before we proceed further. I want you guys make snapshot for each section of this guide. Yep, this will serve as a failsafe incase something went wrong, making it “point of return” for us(yes pun intended). Here’s how to do it:  Click on Machine menu again and select Take Snapshot…Selecting Take Snapshot  After that there will be menu pop that want us to name the snapshot. The name will function as a description for what exactly is this snapshot that we make. So make sure the name and description is informative about what machine situation when the snapshot being made. Making Snapshot Name and DescriptionAnd with that our Ubuntu Server is done we could continue to next step, that is updating our Ubuntu with this commands. But we need to login to our Ubuntu. After done login, execute these steps:Disclaimer for sudo CommandFrom here on we will using A LOT really a lot sudo and it will often asking to enter our root password(same with your user password) that is why I said to use easy to remember password previously. Basically we want to minimize access to our root as little as possible because security concern(shoutout to nubb, thanks for the insight). Okay, on with the guide.  Enter command sudo apt update &amp;&amp; sudo apt upgrade -y. These commands will update and upgrading our existing packages.  Next command sudo apt dist-upgrade -y. Same with previous but the difference is, this command will handle changes of the packages if necessary.  sudo apt install zip unzip -y is the next command, this command installing zip and unzip application for us to be able extracting zip format file and compressing file into it.  Lastly there’s sudo apt install jq -y that used for JSON parsing and transformation. It’s just a fancy words that means our Ubuntu able to interact with JSON format file.With all of that being done, our Ubuntu is ready. Oh, and don’t forget to make snapshot here too just to be extra careful if you guys want.Accessing Machine Through SSHChecking Ubuntu SSHThis section will focusing on SSH thingies, starting from checking it’s status, setting up NAT rules, and many more. I recommend using SSH to simulate remote connection within work environment and to avoid hassle of interacting directly with the VirtualBox(yes, it’s such a pain in the ass). But first thing first, login to the Ubuntu VM and execute this steps:  First, we have to check our Ubuntu SSH status. There’s some possibilities here, either our SSH is working or not. Let’s make sure of it with this command, sudo systemctl status ssh. On my case, the SSH is Active and running like shown below.Making Snapshot Name and DescriptionThis is because we enabling SSH previously during our installation but in case if some of you guys seeing failed or dead status proceed to next steps.  Execute this command sudo apt-get remove --purge openssh-server &amp;&amp; sudo apt-get update &amp;&amp; sudo apt-get install openssh-server. That commands(I use plural here because there’s two command that being fused together with &amp;&amp; operator) basically the commands will first remove and deleting all of the SSH files, service, etc and then installing it again.  When it is done, you guys could run sudo systemctl status ssh and you should see a dead status but don’t worry, execute sudo systemctl enable ssh to enable the service and then systemctl start ssh to start it again. And now next check it’s status again, it should be running now. Don’t forget to make snapshot 📷 .Network And SSH ConfigurationEven with all of that progress previously, we still couldn’t even access the VM directly by their IP, nuh uh that would be too convenient. Welcome to IT world, where if it’s not pain in the ass you should suspect somethings wrong. So how do we SSH the VM then? well let’s get into it:  First head to our VirtualBox Manager menu and select the network option shown below.Selecting Network Menu      After that there will be network menu, in it select NAT Network tab and click on Create icon to create new NAT Networks option.Configuring NAT NetworkFrom the image above, we basically configuring our NAT Network to named “ElasticNetwork” to the network IP within prefix of 192.168.68.0/24 with DHCP server enabled. Below it, we could see there’s Port Forwaring tab where you could add by clicking on the green plus icon and set it’s name, the protocol which is TCP for SSH, host IP being left blank because we want it to be able receiving any IP other from the prefix range, the host port, guest IP with our own Ubuntu IP previously, and lastly the guest port being 22.    Next is to set our network mode for our Ubuntu VM, again we head to Machine &gt; Settings on our VM windows and after that select Network like shown below.Setting Ubuntu Network ConfigurationChange the Attached to with NAT Network, name change it with the network that we make, and lastly I set the Promiscous Mode to Allow All. That make sure it could capture all traffic because well, this is a SIEM lab.Successfully SSH To Ubuntu VMNow all of that done, finally we are able to SSH to the VM and maybe some of you wondering why the IP is 127.0.0.1 and not 192.168.68.5? Well according to the guides that I read, this is possible because the port forwarding rules that we set before that allowing all IP to access SSH to Ubuntu VM through port 2222 which then being redirected to port 22. And 127.0.0.1 itself is another way of saying localhost. I’ve tried to access it with IP 192.168.68.5 but there’s no connection, I still dunno why is that happen even though I’ve tried to add rules for it¯\\_(ツ)_/¯. But oh well, maybe I’m just that noob. Oh and don’t forget to make snapshot 📷.Setting Up ElasticsearchOkay, now the nitty gritty part of it, buckle up and prepare to get your hands dirty because there will be a lot to configure. Oh and we will continue this part on SSH connection.Downloading and Installing ElasticsearchBut before we start downloading Elasticsearch, execute this command sudo apt install apt-transport-https -y to enabling us using APT transport for the HTTPS protocol which is a more secure version of HTTP and for the GPG key of the Elasticsearch. Again, some security stuffs and best practice.Okay, below is screenshot and steps for Elasticsearch installation:Successfully Downloading Elasticsearch  First we download GPG key for the Elasticsearch with command wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -  Next, to see if the repo is exist for our Ubuntu echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" | sudo tee /etc/apt/sources.list.d/elastic-7.x.list.  Now, with all of that done we could update and install Elasticsearch sudo apt update -y &amp;&amp; sudo apt install elasticsearch -yDone and with that we could see the status of Elasticsearch service just like SSH previously with command sudo systemctl status elasticsearch.service.Checking Elasticsearch Service StatusAs you can see, the status is dead and that is because still haven’t enabling the service yet, that will be done on the next section.Configuring Elasticsearch And Checking It’s ConnectionBefore we enabling and running the service, let’s configure Elasticsearch first to suit our need like changing it’s address, protocols, ports, etc.We could do that by entering command sudo nano /etc/elasticsearch/elasticsearch.yml. Let me explain a bit that command, basically we accessing elasticsearch.yml with super user privilege, then choosing our text editor for mine it’s nano but you could choose other text editor, and lastly we specifying the filepath of elasticsearch.yml to be able to open it.After that done, you should see the text editor open up and you could start changing the properties such as shown below:Changing Elasticsearch PropertiesWhat we’re changing here is:  cluster.name to our own wanted name.  network.host to our own IP.  http.port is just commented out because this will be default value of 9200.  discovery.type is the last. This is for Elasticsearch to know whether we run a single node or multiple. More of it can be read hereWith this we then could do these following steps:Enabling Elasticsearch Service And Testing It’s Connection  We first enable the service with sudo systemctl enable elasticsearch.  Then we start it sudo systemctl start elasticsearch.  After that, checking its status sudo systemctl status elasticsearch.  Finally we check the connection with curl --get http://YOUR_OWN_IP:9200. Change YOUR_OWN_IP with the IP you got from DHCP previously.Setting Up KibanaNow, we do the same with Kibana, take a look this screenshot below.Successfully Installing KibanaEnter command sudo apt install kibana -y and wait for it to finish the process. After that we edit the Kibana YML configuration, enter command sudo nano /etc/kibana/kibana.yml disclaimer again, nano is my text editor choice.Changing Kibana YML PropertiesIn here, we change:  server.port, we just commented it out because we will be using it’s default properties.  server.host, change it again to your own IP.  server.name, change it to your desire.Kibana CredentialsAs for Kibana credentials, we just leave it for now. We will back to it in the later steps. And with that, let’s continue next step that is enabling Kibana service.Enabling Kibana Service  We always start with enabling service first, sudo systemctl enable kibana.  Then start it’s service sudo systemctl start kibana.  And finally checking it’s status with sudo systemctl status kibana.And that’s it, we finally installed Elasticsearch and Kibana. You could also checking Elasticsearch and Kibana status both at the same time with sudo systemctl status elasticsearch &amp;&amp; sudo systemctl status kibana. And as always don’t forget to take snapshot 📷.Important NoteThis section is for those who are done for the day of configuring this lab and want to take a rest or any other task. If you want to stop, you have to shutdown all of the component properly, here’s how you do it.Shutting Down System ProperlyThe command used is in order, so first it’s sudo systemctl stop kibana and the second is sudo systemctl stop elasticsearch. This way we could safely shut down our system without worrying the risk to break anything. And another note, kibana.service == kibana &amp;&amp; elasticsearch.service == elastic.And if you guys want to continue again:  sudo systemctl start elasticsearch  sudo systemctl start kibanaSetting Up FilebeatSame thing with previous installation steps, enter sudo apt install filebeat -y, wait for the process to be done and edit the YML file sudo nano /etc/filebeat/filebeat.yml.Inside filebeat.yml file we could adjust and change variety of input received by Elasticsearch from journald, winlog,  Windows event logs, and many more. Winlog helps us captures more log data beyond just logged events. The guides that I follow mostly using Filebeat to collect it’s log, so let’s just focus on that too.What you have to edit in the filebeat.yml is as followsChanging Filebeat HostsWe change that to our own IP that attached to Elasticsearch port 9200. And the next thing to is shown in screenshot below.Disabling Logstash And Checking The Connection To ElasticAccording to the guides that I follows, this is to disable Logstash and to ensure Filebeat correctly connected to Elasticsearch. We use sudo filebeat setup --index-management -E output.logstash.enabled=false 'output.elasticsearch.hosts=[\"YOUR_OWN_IP:9200\"]'.Next we enable the service sudo systemctl enable filebeat.service and we start it sudo systemctl start filebeat.service. Lastly we use curl --get http://YOUR_OWN_IP:9200/_cat/indices?v, that command simply making an API call of the URL and printing it out in verbose but also in column format. Making it more readable. The health here is just indicator of shards whether it’s condition is healthy, caution or danger but as far as I know, this not really mean much. So don’t mind it. Oh and don’t forget to take snapshot 📷.CA Certificate Authorities Configuration and Elastic IPBut why making CA certificate even though Elastic come with it’s own default certificate? To my knowledge the default protocol when access Elastic is HTTP but we want it to be HTTPS, so this is where CA certificate comes in handy. With it, we could enabling HTTPS connection.Now, without further a do, use cd /usr/share/elasticsearch to navigate to elasticsearch directory and enter sudo touch instances.yml and then open the text editor sudo nano instances.yml. After opening the file, insert this propertiesinstances:        - name: \"elasticsearch\"          ip:                  - \"YOUR_OWN_IP\"        - name: \"kibana\"          ip:                  - \"YOUR_OWN_IP\"        - name: \"fleet\"          ip:                  - \"YOUR_OWN_IP\"The IP inserted there is for fleet service able to reach out to it and make CA certificate according to the given IP there. Before exiting, save that file. And yeah, if you want, don’t forget to take snapshot 📷.Time to Create CA Certificate and Other CertificatesSo next is to generate and create other certificates as well, head to cd /usr/share/elasticsearch and we enter command sudo /usr/share/elasticsearch/bin/elasticsearch-certutil ca --pem. That basically mean we using executable file that generate the certs itself, and the encryption format of are X.509. Which is a public key certificates used in TLS/SSL, and HTTPS as well to secure the web, more about that can be found here. As for the --pem itself, it’s a standard file format of storing and sending encrypted key and certificate. And with that, we will get output as follows:CA Certificates OutputOne more thing to know based on that output, we using the instances.yml that made previously to help us generate these certificates. And we compressing it in zip file format, as for the file name I choose to name it elastic-stack-cat.zip but you can choose whatever name convenient to you. With it, we enter sudo unzip elastic-stack-ca.zip to unzipping the file.Nicely done, next steps is to put use of these certificates by using it to generate private key and another certificates. Use this command sudo /usr/share/elasticsearch/bin/elasticsearch-certutil cert --ca-cert ca/ca.crt --ca-key ca/ca.key --pem --in instances.yml --out cert.zip. Follow the screenshot below.Generating Private KeysI’ll go with cert.zip as the name but and I we also need to unzipping this file too. And after that we need to use command sudo unzip cert.zip to unzipping it.Unzipping Cert And Making Directory CertNotice that Linux making it’s own directory after unzipping it, because of that the next thing is to move all of those certificates to our newly created directory. We make directory to store all those certificates with sudo mkdir certs, after the directory created use sudo mv /usr/share/elasticsearch/elasticsearch/* certs/ first, then sudo mv /usr/share/elasticsearch/kibana/* certs/, lastly sudo mv /usr/share/elasticsearch/fleet/* certs/. After the copying steps done, we then use sudo mkdir -p /etc/elasticsearch/certs/ca &amp;&amp; sudo mkdir -p /etc/kibana/certs/ca &amp;&amp; sudo mkdir -p /etc/fleet/certs/ca to make CA own directory just like screenshot below where we could store our CA certificates.The CA directory has been made, now we copy CA certs and other certificates to their respective directory with these command.  sudo cp ca/ca.* /etc/elasticsearch/certs/ca &amp;&amp; sudo cp ca/ca.* /etc/kibana/certs/ca &amp;&amp; sudo cp ca/ca.* /etc/fleet/certs/ca  sudo cp certs/elasticsearch.* /etc/elasticsearch/certs/ &amp;&amp; sudo cp certs/kibana.* /etc/kibana/certs/ &amp;&amp; sudo cp certs/fleet.* /etc/fleet/certs/I used list command to check the directory and noticing that there’s still certs directories left around. So lastly just like mom used to say, clean up after playing with your toy and like a good kid we are we clean up our mess with sudo rm -r elasticsearch/ kibana/ fleet/Moving The Certificates FilesAlso don’t forget to take snapshot 📷 hehehehe.Best Practice For SIEM DeploymentThis section is for us to implement best practice when we deploying SIEM is to have least privilege within our systems and according nubb, the PrintNightmare threat is still lurking rampant out there so gotta be careful. More about that you could read here.And this also correlate with our usage of sudo command where we want the least privilege. With that in mind, head to cd /usr/share and execute commands:  sudo chown -R elasticsearch:elasticsearch elasticsearch/,  which will recursively set the ownership of the elasticsearch.  sudo chown -R elasticsearch:elasticsearch /etc/elasticsearch/certs/caThe first command let us set the elasticsearch directory to Elasticsearch and also change its group to elasticsearch that helps us limit permission to again, Elasticsearch. Second command is doing the same thing but for ca directory.Best Practice SIEM DeploymentBased on that screenshot, the command being used is sudo openssl x509 -in /etc/elasticsearch/certs/elasticsearch.crt -text -noout. Where there’s couple of command flag that I want to explain to the best of my understanding. -in flag used to specifying the certificate to check out, -text to make the output in readable format and lastly -noout is basically minimizing the gibberish encoded message to help further making it more readable.Setting Up HTTPS Connection With the CertificatesFirst head to the kibana.yml file with sudo nano /etc/kibana/kibana.yml and we copy this YML script below:server.ssl.enabled: trueserver.ssl.certificate: \"/etc/kibana/certs/kibana.crt\"server.ssl.key: \"/etc/kibana/certs/kibana.key\"elasticsearch.hosts: [\"https://YOUR_OWN_IP:9200\"]elasticsearch.ssl.certificateAuthorities: [\"/etc/kibana/certs/ca/ca.crt\"]elasticsearch.ssl.certificate: \"/etc/kibana/certs/kibana.crt\"elasticsearch.ssl.key: \"/etc/kibana/certs/kibana.key\"server.publicBaseUrl: \"https://YOUR_OWN_IP:5601\"xpack.security.enabled: truexpack.security.session.idleTimeout: \"30m\"xpack.encryptedSavedObjects.encryptionKey: \"min-32-byte-long-strong-encryption-key\"Remember to change YOUR_OWN_IP to your own IP and also some of you might notice that I input 10.0.2.15 as the IP right? Well that just me being dumbo and you will see what happen when you don’t correctly set the IP in the next section. And all of These properties is all within the kibana.yml but it will be pain in the ass to search each of it and editing them so yeah, just copy that. And it will look like this.Configuring Kibana SSLThe properties here somewhat self explanatory, the public base URL is our Kibana address, and others is for our encryption thingies.  But albeit, I agree with nubb here, I was also confused when first time handling the certificates and encryption stuff of ELK stack. But basically it’s just how Kibana telling Elastic that the communication between them is secured with various encrypted certificates and that Kibana make sure, it is trusted Kibana that talking with Elasticsearch.Or, cmiiw that is just how I understands these whole shenanigan. Proceed to next step that is basically the same but for elasticsearch.yml. sudo nano /etc/elasticsearch/elasticsearch.yml and start add the properties.xpack.security.enabled: truexpack.security.authc.api_key.enabled: truexpack.security.transport.ssl.enabled: truexpack.security.transport.ssl.verification_mode: certificatexpack.security.transport.ssl.key: /etc/elasticsearch/certs/elasticsearch.keyxpack.security.transport.ssl.certificate: /etc/elasticsearch/certs/elasticsearch.crtxpack.security.transport.ssl.certificate_authorities: [\"/etc/elasticsearch/certs/ca/ca.crt\"]xpack.security.http.ssl.enabled: truexpack.security.http.ssl.verification_mode: certificatexpack.security.http.ssl.key: /etc/elasticsearch/certs/elasticsearch.keyxpack.security.http.ssl.certificate: /etc/elasticsearch/certs/elasticsearch.crtxpack.security.http.ssl.certificate_authorities: [\"/etc/elasticsearch/certs/ca/ca.crt\"]Configuring Elasticsearch SSLI’ll leave the official documentation for you to read more information and nubb explanation about this SSL things. And I agree again with nubb here, are the Elastic devs high on something when making the script to not use quotation for file path? Only God and them knows why.With that said and done, we have to sudo systemctl restart elasticsearch and next sudo systemctl restart kibana on respective order for enabling us applying the changes. With that don’t forget to take snapshot 📷.Testing Connections to ElasticsearchNow let’s what all of these headache got us. Execute the command curl --get https://YOUR_OWN_IP:9200.Testing ConnectionsNotice that it failed for the first time? Yeah, like I said, if it’s too good to be true in IT, chances are you missing something. Apparently the reason for the failed testing is because while the Kibana and Elasticsearch trust each other, the browser is don’t.Kinda like when you bring your other friend to your other friend, of course they still don’t trust each other instantly right?. That is why as their friend you would say “Trust me bro, this kid is good” and that is exactly what we would do with the command curl --get https://10.0.2.15:9200 --insecure | jq the --insecure is the “trust me bro” in this case where we bypassing it temporarily and as for the jq telling the output to make on JSON format. This why we install JSON packages previously.Creating CredentialsNow one more thing before we see ELK stack frontend, we have to generate credential in order to login to ELK stack. Here’s the command sudo /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto and basically this means we let the Elastic generate it for us. All the information about it can be found here. Here’s what it will look likeGenerated CredentialsAnd don’t forget to save all of that credentials, oh and also don’t change the elastic.username but just change password of it to the generated password previously. The ELK stack will stuck on endless loop if you don’t change those two properties. The elastic username used for superuser and it basically the core user of ELK. Changing it will make ELK stack error.Now with all of that restart the ELK stack with sudo systemctl restart kibana &amp;&amp; sudo systemctl restart elasticsearch.Now we ready to login ELK stack and using it. But before that, don’t forget to take snapshot 📷.Rule Forwarding For ELKOne last setting that we need to set before connecting to ELK stack is to add another port forwarding rules but this time for the connection to ELK stack. For the sake of simplicity, here’s the step below.Adding ELK Connection In Port Forwarding RuleTo add the rule above, we head to Virtual Box Manager just like previously we add SSH rule and then select Network &gt; NAT Network &gt; select Port Forwarding tab &gt; and click on the green plus icon. Just like that, we add the rule just like shown picture above. Finally, with this we could connecting to the ELK stack.Moment of TruthOh and if you just start again don’t forget to start the ELK stacks. I’ll explain it further but take a look this screenshot.Let The Browser Trust The ConnectionThe browser showing that because it still don’t trust the ELK stack connection yet and warning us. But we know this is a trusted connection and let the browser know that by clicking Advanced… and Accept the Risk and ContinueKibana Server Not Ready YetInteresting that we access it with https://127.0.0.1:5601 but not with YOUR_OWN_IP. Why is that? Well, that is because the ELK stack hosted on your own IP but not accessible publicly, that is why accessing it locally with 127.0.0.1. And what’s that not ready yet message? Welp, just like the name suggested, the system is indeed not ready yet, maybe warm up or smth ¯/_(ツ)_/¯.We a bit and….. voilà, the ELK stack front ladies and gentlemen. Phew… that was long journey huh? Nicely done guys.ELK Stack Finally Able To UseWhat’s About the Wrong IP Before?This last section is to explanation the silliness that made previously by entering wrong IP. Basically what will happen is that ELK stack will infinitely on Not Ready Yet status and to fix it?Infinite Kibana Server Not Ready YetTo fix it we have to change it back to your correct IP in kibana.yml like shown in the picture below.Change that 10.0.2.15 With Your Own IpAnd that 10.0.2.15 needed to be change to your own IP. With that, you finally fixing the Not Ready Yet status.Last Words and ReferencesThank you so much for spending your time following and reading through all of the yapping from me. I really hope this guide helping you on some way with your learning journey of cyber security stuff. And with this final words, I want to reference some of the sources that I used in this part of guide.            References                  nubb guide on github. Elastic-SIEM-Setup              LevelEffect guide by Will Nissler. How to Set Up Your Own Home Lab with ELK              ELK stack definition. elastic              Elastic discovery documentation. Discovery and cluster formation settings              X.509 definition. X.509              PrintNightmare information. Demystifying The PrintNightmare Vulnerability              SSL and other security information of ELK stack documentation. Security settings in Elasticsearch              PEM certificate formats information. Privacy-Enhanced Email              Kibana endless not ready yet status fix. Kibana server is not ready yet [closed]      "
  },
  
  {
    "title": "Detecting Windows Logon Events 4624-4625 With Elasticsearch As a SIEM",
    "url": "/posts/detecting-windows-logon-vents-4624-4625-with-elasticsearch-as-a-lightweight-self-managed-siem/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, windows, security events, detection",
    "date": "2025-04-12 13:40:00 +0700",
    





    
    "snippet": "Detecting My First EventFirst time I’m diving into cyber security and being tasked to explore Elasticsearch and it’s functionality, that is where I’m really in a dark as to how and where to begin w...",
    "content": "Detecting My First EventFirst time I’m diving into cyber security and being tasked to explore Elasticsearch and it’s functionality, that is where I’m really in a dark as to how and where to begin with this journey ahead. But then I watch some videos on youtube and in it Elasticsearch can be used for events detection. At that time, there’s a light bulb lights up on my mind because I thought to myself “How about I simulate a basic events using this local Elasticsearch that I’ve just installed and see what is it looks like in the Kibana? And along the way I could also learn other cyber security concept. Two birds with one stone” I said to myself.My Thought Process Looks like This Fr FrSo with this post I’ll explain and walk through about what is my setup for this scenario, the configuration for the setup, what kind of queires for logs that I used, what I learn from this little experiment, and some bits about what I should know more. So if you feels like just like me, new to Elasticsearch as SIEM or maybe just cyber security in general especially blue team, I hope this experience of mine helps you.What Is Event ID 4624 &amp; 4625? And Why Need To Detect Them?So for starter, all of the processes and events that is happening inside the computer are being recorded and stored in a form of logs that being specified by the system’s audit policy. And that is called Windows Security Logs, where it’s offer an immense amount of insight into what’s happening on a computer/machine. Our so called ‘main highlight’ for today is event ID 4624 &amp; 4625.But before we delve more in event ID 4624 &amp; 4625, let me explain what exactly does ‘ID’ means here. So in term of windows event logging, ‘ID’ is referring to unique identifier that uniquely identifies particular events that help define it’s description so that viewers can present these description to the user and thus helping user solving their own problems.And with that said definition we could finally explain and understand better what are those two events exactly.  ✅ Event ID 4624 – Successful Logon          Generated when user logs on successfully to a computer/machine.      This is very Useful for understanding who logged in, from where, and using what method.        ❌ Event ID 4625 – Failed Logon          Generated when a logon attempt fails by user.      Extremely useful for detecting brute force attacks, incorrect password attempts, or even internal misuse.      And with that information and adding documentation from Microsoft’s official documentation and the event ID reference, we could at least expect the information would be provided with fields like:      TargetUserName: Who tried to log in        IpAddress: From where        LogonType: What method (RDP, Interactive, Network, etc.)        FailureReason: For 4625, why the login failed        ProcessName: What app triggered the login  As we can see, the information really essential for us to help spot some patterns, suspicious behaviour or unsual acitivity in general.The Necessary ToolsFor the stack that I used in this little project to gather, store, and visualize the logs being gathered are as follows:      Winlogbeat (Agent installed on Windows host)        Elasticsearch (Storage &amp; querying engine)        Kibana (Dashboard for data exploration)        Windows Installed Device(Generating real login events)  This stack is a part of the Elastic Stack (ELK), which is widely used for log management and lightweight SIEM purposes.Step-by-Step Setup  ✍️ Step 1: Install Winlogbeat on the Windows HostDownload and install Winlogbeat from the official Elastic downloads page.Edit the configuration file (winlogbeat.yml):winlogbeat.event_logs:  - name: Security    event_id: 4624, 4625Specify the destination for Elasticsearch:output.elasticsearch:  hosts: [\"http://localhost:9200\"]Enable the Winlogbeat service:.\\winlogbeat.exe install-serviceStart-Service winlogbeatNow that the winlog is set, we then can proceed to next step. Connecting the winlog to Elasticsearch and Kibana.      🔗 Step 2: Connect Elasticsearch and Kibana                  Ensure Elasticsearch is up and running: http://localhost:9200                    Open Kibana: http://localhost:5601                    Navigate to Stack Management &gt; Index Patterns and create a pattern like winlogbeat-*            Start Querying Those Windows Logon Events in KibanaWith all of the setup being done and Kibana is ready to use, head to Discover menu on the Kibana like in the picture below.Kibana Discover Working Properly Showing Logs &amp; EventsAfter inside the Kibana Discover tab, we could see all of what’s happening inside our machine/computer. This will give us a clear information to pin point the threats, adverseries, and any other suspicious activies.The next steps to see the wanted events is to input the queries. We can input the queries like shown in the picture below alongside the necessary queries.Successfully Filtering Event ID 4624The language being used in the picture above is called KQL(Kibana Query Language) but before we continue with the event. Let me briefly expalain what is KQL in my understanding.  🧠 So What is KQL (Kibana Query Language)?It’s a set of search syntax in Kibana to filter and explore log data within Kibana that is designed to be intuitive and user-friendly for analysts to be able build queries without writing complex raw Elasticsearch DSL.They way it being structured is by having field-based searches, logical operators, and even wildcards to filter events quickly. Here’s the composition of it explained in table form:            Component      Example      Description                  Field match      event.code: \"4624\"      Match entries where event.code is exactly 4624.              AND / OR operators      event.code: \"4625\" AND winlog.event_data.LogonType: \"10\"      Combines multiple conditions. AND = both must match. OR = either one.              Grouping values      event.code: (\"4624\" or \"4625\")      Matches either value in a field.              Wildcard search      TargetUserName: \"admin*\"      Matches values that start with admin (e.g., administrator, admin1).              Phrase match      ProcessName: \"C:\\\\Windows\\\\System32\\\\svchost.exe\"      Searches for exact phrases, often used with file paths.              Negation      NOT TargetUserName: \"SYSTEM\"      Excludes logs where username is SYSTEM.              Time range      @timestamp &gt;= \"now-1d/d\"      Shows only logs from the last day. Supports relative time filtering.      How To Simulate Event ID 4625With the knowledge of how KQL structured and compose we can now then simulate how to make the event appear. Because event 4624 is a success logon, for this section I try to emulate event 4625. This is because currently I’m using one machine/computer only.There’s various way to emulate it, here are some of it:  Using Runas with Wrong Password          Open CMD or PowerShell      Run the following command on it runas /user:fakeuser cmd      When prompted, enter any password (wrong is fine).        Create a Dummy Local Account, Then Log in WrongIf it need to be more realism:                  Create a dummy user using PowerShell(run as administrators):net user admin P@ssword123 /add            Now try logging in with this account using the wrong password:                  Lock screen (Win + L) → switch user → admin                    Try logging in with wrong credentials:                  Use something like admin123 as the username and any wrong password.                    Delete the account after testing:                  net user admin /delete                     What Is It Looks Like Inside Discover in Kibana?And with that being done, we then can proceed to monitor the activity using some of the queries. Here’s some explame of it:  All successful logins    event.code: \"4624\"        Failed logins via local    event.code: \"4625\" AND winlog.event_data.LogonType: \"2\"        Filter out noise from system accounts    NOT TargetUserName: \"SYSTEM\" AND event.code: \"4625\"        Failed login attempts where the username starts with “admin”    user.name : \"admin\" AND event.code: \"4625\"        Failed logins from the last hour    @timestamp &gt;= \"now-1h/h\" AND event.code: \"4625\"      And that’s some queries that I tried to shows the regarding events on this little experiment of mine.Insights &amp; Lessons LearnedHere’s what I learned while building this setup:      Elasticsearch is flexible: You can query just about anything if you understand the schema.        Fields matter: Understanding LogonType, TargetUserName, and ProcessName can surface important patterns.        Kibana makes learning fun: Seeing the data visually helped me internalize the concepts faster.        Winlogbeat is efficient: It shipped the logs reliably with minimal config and resource usage.        Also: filtering out logon attempts from Anonymous users or from NT AUTHORITY\\SYSTEM helped clean up the data.  What I Still Don’t Know (Yet)This little experiment raised more questions that I’d like to explore:      How can I set up alerts for excessive 4625s in a short time? (e.g., brute force detection)        What is the best way to enrich logs with user info or location?        Can I use Elastic Security (SIEM app in Kibana) to build detection rules easily?        What about other logon-related event IDs like 4768 (Kerberos ticket request) or 4771 (pre-auth failure)?  Conclusion: My First Step into Detection EngineeringThis post helped me move from theoretical understanding to practical skill-building with Elasticsearch and Windows logs. I now better understand:      How login events work on Windows        How to query and filter them using KQL        How to use Elasticsearch and Kibana as lightweight SIEM components        Next, I want to explore Elastic Security, and maybe even try shipping Linux logs or network traffic into the stack. The possibilities are wide open.  If you’re on the same journey or have tips for me, I’d love to hear them. What logs or events do you think I should dive into next?Thanks for reading! If you found this helpful, check out my [first post] where I talk about the installation pains and how I made sense of the Elastic ecosystem as a beginner."
  },
  
  {
    "title": "Elastic Self-managed Installation",
    "url": "/posts/elastic-self-managed-installation/",
    "categories": "Tutorial, SIEM, ELK Stack",
    "tags": "elasticsearch, siem, blog, itsecurity",
    "date": "2025-04-11 16:25:00 +0700",
    





    
    "snippet": "What Makes Me StartI just started my first work as SOC Analyst back in march 2025. When I first started, my manager asking me to install and play around with Elasticsearch. An application that I ne...",
    "content": "What Makes Me StartI just started my first work as SOC Analyst back in march 2025. When I first started, my manager asking me to install and play around with Elasticsearch. An application that I never heard before. My first thought is just “meh, it’s just another apps, what could be so difficult about it”.Basically What My Experience Looks likeOh boy was I so wrong. Elastic is a giant to learn in itself, so much of features, processes and many other that I should learn from the scratch. So in this post, I would like to begin with how I setting up my Elasticsearch. Hope you guys learn something from this post.What is Elasticsearch exactly?So, what’s about this ‘Elasticsearch’ being so difficult to me when I first encounter it, is that there’s three in one shenanigans for this applications. Imagine your system is screaming — “Things are breaking!” — but all the signs are scattered across thousands of log files in weird folders. You don’t have time to read all of them. Elasticsearch is like a giant smart search engine that stores and analyzes those logs fast. It helps you make sense of what’s happening in your system.So here’s a diagram to visualize how Elasticsearch worksDiagram of How Elasticsearch WorksAs you can see on the diagram, there’s so many components accompanying Elasticsearch itself. And from my experience alone here’s some of steps or things that I learn during my trial and errors.What I learned:  Always follow the correct order of installation: Elasticsearch → Kibana → Fleet.  Keep your tokens and fingerprints handy, don’t forget them.  Watch logs closely during setup — they often explain what’s wrong.The Installation ProceduresSo with that in mind, the steps that I follow to installing Elasticsearch and Kibana is this video below by Wasay Tech Tips.What’s Missing From The InstallationBut, here’s the catch. After I done following and installing Elasticsearch and Kibana based on that video, I still don’t have log or any data showing up on my Elasticsearch. And turns out, I need to install the Agent for it to be working properly.So the way Elasticsearch works is that the node or in this case my laptop to be able to sent log or data to Elasticsearch is by having agent installed on my device. So that is what I do, installing the agent and here’s how I do it.  First you need to install the agent from the official Elasticsearch website here.      After downloading it, extract the file and head inside the file of the downloaded agent file and then type cmd on the address bar and press enter. This will make cmd open on that file path.    The cmd shows up, don’t enter installation command yet. Head to your Elasticsearch localhost shown in the picture below.  In the Fleet menu, click on “Add Fleet Server”.  Next, the add server menu shows up and this is where we could select where we wanted to put our fleet server. In my case, I put on the localhost address with port 8220(you could get the localhost address from the configuration yml file) and click continue.  The fleet server location has been set, next step is to copy the installation command line to the previously started cmd. But keep in mind that for me, this command line from my localhost Elasticsearch is missing some key command line that I will show on the next step.  And for the final step of agent installation is to add this command to the cmd.    .\\elastic-agent.exe install ^  --fleet-server-es=https://192.168.56.1:9200 ^  --fleet-server-service-token=AAEAA... ^  --fleet-server-policy=6be1b061-868b..... ^  --fleet-server-es-ca-trusted-fingerprint=f54c4b...... ^  --fleet-server-port=8220        For the server service token, server policy, and ca trusted fingerprint it should be generated automatically if you on fresh installment. For my case, this is because I have previously installing agent and uninstall it again. So you should be getting it automatically if it’s firt time installment.Next, paste it on the cmd like picture below.  And with that, the Elasticsearch installation should be completed now.Setting Up IntegrationBut we’re not done yet, we still need those integration for our Elasticsearch to work properly. So next up is, setting the integration.  From the navigation side bar menu, we can find the Integration tab. Click that and we will be routed to integration menu.  Once we in the integration menu, we could then search for the relevant integration that satisfy our needs. For this tutorial, I need Kibana for visualizing my log so that I could understand all of the data that being collected. We could use the search bar for finding Kibana. And once Kibana found, click on it.  Now that we’re in Kibana installation menu, click “Add Kibana” button. This way we can begin the installation process.  Usually, all the previous steps above should be set by default. The one that we want to set however is “Where to add this integration” option. This option asking us in what agent policies that we want to put out integration. For me because my fleet server is on agent policies 8, I have to put my Kibana on that so it can work with the fleet server.  After that, it will give prompt like this. Just click “Save and deploy changes”.  And wait for the installation to completed and voilà our integration installed successfully.But What Agent Policies?Now you might be wondering, “What is this policies exactly? Are they like rules? Or what?”. Now when I first installed Elastic I also have the same question and turns out, Agent Policies are like a set of instructions that tells each Elastic Agent what to do, what to collect, and where to send it. It’s like handing your agent a to-do list:  What logs or metrics to monitor (e.g., Windows Event Logs, system metrics)  Which integrations are enabled (like Winlogbeat, Filebeat, System, etc.)  Where the data goes (usually to Elasticsearch)  Whether the agent can auto-update or be centrally managedEach device running an Elastic Agent is tied to one of these policies. So if I have 5 computers with Elastic Agents and they’re all doing the same job (like collecting Windows logs), I can put them under the same policy. That way, if I change something in that policy — boom 💥 — it applies to all of them at once. Super convenient.Elastic even lets you have multiple policies so you can customize agents differently for servers, workstations, cloud VMs, etc.The DashboardAnd with all of the installation process, here’s what Elasticsearch dashboard look like if the installation succeeded.Final ThoughtsThis blog isn’t meant to be perfect. It’s real. If you’re learning Elastic and security monitoring with just one machine and zero prior experience, I hope this helps. My goals for writing this blog is to help others and future me in-case need to start from zero again on the elastic installation, so this blog could make as an knowledge based to others and me too.Thank you so much for reading this and have a nice day where ever you are."
  }
  
]

